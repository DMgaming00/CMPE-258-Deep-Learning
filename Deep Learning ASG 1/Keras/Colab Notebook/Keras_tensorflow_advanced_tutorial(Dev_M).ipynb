{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zEkrSo01WcC7"
      },
      "source": [
        "# TensorFlow & Keras Advanced: Custom Layers and Deep Architectures\n",
        "\n",
        "## Part 2: Building Complex Operations from Scratch\n",
        "\n",
        "---\n",
        "\n",
        "In Part 1, we learned TensorFlow fundamentals, GradientTape basics, and the high-level Keras API. Now we go deeper!\n",
        "\n",
        "### What You'll Learn\n",
        "\n",
        "| Part | Topic | Key Concepts |\n",
        "|------|-------|-------------|\n",
        "| **I** | Advanced GradientTape | Nested tapes, Jacobians, custom gradients |\n",
        "| **II** | Building Ops from Scratch | Convolution, pooling, normalization by hand |\n",
        "| **III** | Custom Layers (Primitives) | Build layers using only tf.Variable |\n",
        "| **IV** | Custom Keras Layers | Proper subclassing with `build()` and `call()` |\n",
        "| **V** | Advanced Architectures | Residual blocks, attention, custom normalizations |\n",
        "| **VI** | Custom Training Loops | Full control over training with GradientTape |\n",
        "| **VII** | Practical Demos | Real-world examples with custom components |\n",
        "\n",
        "---\n",
        "\n",
        "*\"To understand the framework, build it from scratch.\"*"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hQaXbL0qWcC7",
        "outputId": "d0d6a3a4-6a44-4dec-a97f-87daf8102240"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "TensorFlow Version: 2.19.0\n",
            "Keras Version:      3.10.0\n",
            "GPU Available:      False\n",
            "\n",
            "Ready for Advanced TensorFlow & Keras!\n"
          ]
        }
      ],
      "source": [
        "# ============================================================================\n",
        "#                           SETUP & IMPORTS\n",
        "# ============================================================================\n",
        "\n",
        "import tensorflow as tf\n",
        "import keras\n",
        "from keras import layers, Model, Sequential\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from typing import List, Tuple, Optional, Callable, Union\n",
        "\n",
        "# Beautiful plots\n",
        "plt.style.use('seaborn-v0_8-whitegrid')\n",
        "plt.rcParams['figure.figsize'] = (10, 6)\n",
        "plt.rcParams['font.size'] = 12\n",
        "\n",
        "# Check versions\n",
        "print(f\"TensorFlow Version: {tf.__version__}\")\n",
        "print(f\"Keras Version:      {keras.__version__}\")\n",
        "print(f\"GPU Available:      {len(tf.config.list_physical_devices('GPU')) > 0}\")\n",
        "\n",
        "# GPU memory growth\n",
        "gpus = tf.config.list_physical_devices('GPU')\n",
        "if gpus:\n",
        "    for gpu in gpus:\n",
        "        tf.config.experimental.set_memory_growth(gpu, True)\n",
        "\n",
        "# Reproducibility\n",
        "tf.random.set_seed(42)\n",
        "np.random.seed(42)\n",
        "\n",
        "print(\"\\nReady for Advanced TensorFlow & Keras!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0tItbO-4WcC8"
      },
      "source": [
        "---\n",
        "\n",
        "# Part I: Advanced GradientTape Patterns\n",
        "\n",
        "## Beyond Basic Gradient Computation\n",
        "\n",
        "In Part 1, we used GradientTape for simple gradients. Now we'll explore:\n",
        "\n",
        "- **Nested tapes** for higher-order derivatives\n",
        "- **Jacobian and Hessian** computation\n",
        "- **Custom gradients** for non-differentiable operations\n",
        "- **Gradient clipping** and manipulation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "W9-VhzBFWcC8",
        "outputId": "d450f3da-8a94-4427-94ee-a00588cfefd3"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "============================================================\n",
            "       NESTED TAPES: HIGHER-ORDER DERIVATIVES\n",
            "============================================================\n",
            "\n",
            "f(x) = x^4, evaluated at x = 2.0\n",
            "\n",
            "f(x)    = 16.0         (expected: 16)\n",
            "f'(x)   = 32.0         (expected: 32 = 4*8)\n",
            "f''(x)  = 48.0         (expected: 48 = 12*4)\n",
            "f'''(x) = 48.0         (expected: 48 = 24*2)\n"
          ]
        }
      ],
      "source": [
        "# ============================================================================\n",
        "#                    NESTED GRADIENTTAPES: HIGHER-ORDER DERIVATIVES\n",
        "# ============================================================================\n",
        "\n",
        "print(\"=\"*60)\n",
        "print(\"       NESTED TAPES: HIGHER-ORDER DERIVATIVES\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "# Example: Compute first, second, and third derivatives\n",
        "# f(x) = x^4\n",
        "# f'(x) = 4x^3\n",
        "# f''(x) = 12x^2\n",
        "# f'''(x) = 24x\n",
        "\n",
        "x = tf.Variable(2.0)\n",
        "\n",
        "with tf.GradientTape() as tape3:\n",
        "    with tf.GradientTape() as tape2:\n",
        "        with tf.GradientTape() as tape1:\n",
        "            y = x ** 4\n",
        "        dy_dx = tape1.gradient(y, x)      # First derivative: 4x^3\n",
        "    d2y_dx2 = tape2.gradient(dy_dx, x)    # Second derivative: 12x^2\n",
        "d3y_dx3 = tape3.gradient(d2y_dx2, x)      # Third derivative: 24x\n",
        "\n",
        "print(f\"\\nf(x) = x^4, evaluated at x = {x.numpy()}\")\n",
        "print(f\"\")\n",
        "print(f\"f(x)    = {y.numpy():.1f}         (expected: 16)\")\n",
        "print(f\"f'(x)   = {dy_dx.numpy():.1f}         (expected: 32 = 4*8)\")\n",
        "print(f\"f''(x)  = {d2y_dx2.numpy():.1f}         (expected: 48 = 12*4)\")\n",
        "print(f\"f'''(x) = {d3y_dx3.numpy():.1f}         (expected: 48 = 24*2)\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FdSJanpUWcC8",
        "outputId": "328ffc40-17b6-46f7-af7a-e348a92a6ccb"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "============================================================\n",
            "              JACOBIAN COMPUTATION\n",
            "============================================================\n",
            "\n",
            "x = [1. 2. 3.]\n",
            "y = f(x) = [x1^2, x1*x2, sin(x3)] = [1.      2.      0.14112]\n",
            "\n",
            "Jacobian (3x3):\n",
            "[[ 2.         0.         0.       ]\n",
            " [ 2.         1.         0.       ]\n",
            " [ 0.         0.        -0.9899925]]\n",
            "\n",
            "Expected Jacobian:\n",
            "  [2*x1,   0,      0   ]   = [2,   0,     0     ]\n",
            "  [x2,     x1,     0   ]   = [2,   1,     0     ]\n",
            "  [0,      0,  cos(x3) ]   = [0,   0,  -0.9900]\n"
          ]
        }
      ],
      "source": [
        "# ============================================================================\n",
        "#                    JACOBIAN COMPUTATION\n",
        "# ============================================================================\n",
        "\n",
        "print(\"=\"*60)\n",
        "print(\"              JACOBIAN COMPUTATION\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "# The Jacobian is the matrix of all first-order partial derivatives\n",
        "# For f: R^n -> R^m, the Jacobian J is m x n where J[i,j] = df_i/dx_j\n",
        "\n",
        "x = tf.Variable([1.0, 2.0, 3.0])\n",
        "\n",
        "with tf.GradientTape() as tape:\n",
        "    # Vector function: f(x) = [x1^2, x1*x2, sin(x3)]\n",
        "    y = tf.stack([\n",
        "        x[0] ** 2,\n",
        "        x[0] * x[1],\n",
        "        tf.sin(x[2])\n",
        "    ])\n",
        "\n",
        "# Compute full Jacobian\n",
        "jacobian = tape.jacobian(y, x)\n",
        "\n",
        "print(f\"\\nx = {x.numpy()}\")\n",
        "print(f\"y = f(x) = [x1^2, x1*x2, sin(x3)] = {y.numpy()}\")\n",
        "print(f\"\\nJacobian (3x3):\")\n",
        "print(f\"{jacobian.numpy()}\")\n",
        "\n",
        "print(f\"\\nExpected Jacobian:\")\n",
        "print(f\"  [2*x1,   0,      0   ]   = [2,   0,     0     ]\")\n",
        "print(f\"  [x2,     x1,     0   ]   = [2,   1,     0     ]\")\n",
        "print(f\"  [0,      0,  cos(x3) ]   = [0,   0,  {np.cos(3):.4f}]\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0_a3dkRlWcC8",
        "outputId": "f707e66d-22cc-4a24-aabb-7846dc2f7d60"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "============================================================\n",
            "              HESSIAN COMPUTATION\n",
            "============================================================\n",
            "\n",
            "f(x, y) = x^2*y + y^3, at (x, y) = (1.0, 2.0)\n",
            "f = 10.0\n",
            "\n",
            "Gradient: [ 4. 13.]\n",
            "  Expected: [2xy, x^2 + 3y^2] = [4, 13]\n",
            "\n",
            "Hessian:\n",
            "[[ 4.  2.]\n",
            " [ 2. 12.]]\n",
            "  Expected:\n",
            "  [2y,  2x ]   = [4, 2]\n",
            "  [2x,  6y ]   = [2, 12]\n"
          ]
        }
      ],
      "source": [
        "# ============================================================================\n",
        "#                    HESSIAN COMPUTATION\n",
        "# ============================================================================\n",
        "\n",
        "print(\"=\"*60)\n",
        "print(\"              HESSIAN COMPUTATION\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "# The Hessian is the matrix of second-order partial derivatives\n",
        "# H[i,j] = d^2f / (dx_i dx_j)\n",
        "\n",
        "x = tf.Variable([1.0, 2.0])\n",
        "\n",
        "with tf.GradientTape() as tape2:\n",
        "    with tf.GradientTape() as tape1:\n",
        "        # Scalar function: f(x, y) = x^2*y + y^3\n",
        "        f = x[0]**2 * x[1] + x[1]**3\n",
        "    grad = tape1.gradient(f, x)  # [2xy, x^2 + 3y^2]\n",
        "hessian = tape2.jacobian(grad, x)\n",
        "\n",
        "print(f\"\\nf(x, y) = x^2*y + y^3, at (x, y) = ({x[0].numpy()}, {x[1].numpy()})\")\n",
        "print(f\"f = {f.numpy()}\")\n",
        "print(f\"\\nGradient: {grad.numpy()}\")\n",
        "print(f\"  Expected: [2xy, x^2 + 3y^2] = [4, 13]\")\n",
        "print(f\"\\nHessian:\")\n",
        "print(f\"{hessian.numpy()}\")\n",
        "print(f\"  Expected:\")\n",
        "print(f\"  [2y,  2x ]   = [4, 2]\")\n",
        "print(f\"  [2x,  6y ]   = [2, 12]\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "srxJFP_KWcC9",
        "outputId": "f65cf912-fbe9-4cc1-8fde-d0d9e4468d98"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "============================================================\n",
            "              CUSTOM GRADIENTS\n",
            "============================================================\n",
            "\n",
            "Input: [3. 4.]\n",
            "Gradient (clipped to norm 1.0): [0.70710677 0.70710677]\n",
            "Gradient norm: 1.0000\n"
          ]
        }
      ],
      "source": [
        "# ============================================================================\n",
        "#                    CUSTOM GRADIENTS\n",
        "# ============================================================================\n",
        "\n",
        "print(\"=\"*60)\n",
        "print(\"              CUSTOM GRADIENTS\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "# Sometimes you need to define custom gradients:\n",
        "# - For non-differentiable operations (like argmax)\n",
        "# - For numerical stability\n",
        "# - For custom backward passes (like straight-through estimators)\n",
        "\n",
        "@tf.custom_gradient\n",
        "def clip_gradient_norm(x, clip_value=1.0):\n",
        "    \"\"\"\n",
        "    Forward: identity function\n",
        "    Backward: clip gradient norm\n",
        "    \"\"\"\n",
        "    def grad(dy):\n",
        "        # Clip the incoming gradient\n",
        "        norm = tf.norm(dy)\n",
        "        clipped = tf.cond(\n",
        "            norm > clip_value,\n",
        "            lambda: dy * clip_value / norm,\n",
        "            lambda: dy\n",
        "        )\n",
        "        return clipped  # Only return gradient for 'x'\n",
        "    return x, grad\n",
        "\n",
        "# Test custom gradient\n",
        "x = tf.Variable([3.0, 4.0])  # Gradient will have norm 5 (3-4-5 triangle)\n",
        "\n",
        "with tf.GradientTape() as tape:\n",
        "    y = clip_gradient_norm(x, clip_value=1.0)\n",
        "    loss = tf.reduce_sum(y)  # Gradient would be [1, 1] but we pass [3, 4]\n",
        "\n",
        "# Manually set upstream gradient to [3, 4]\n",
        "grad = tape.gradient(loss, x)\n",
        "print(f\"\\nInput: {x.numpy()}\")\n",
        "print(f\"Gradient (clipped to norm 1.0): {grad.numpy()}\")\n",
        "print(f\"Gradient norm: {tf.norm(grad).numpy():.4f}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hl8-8EpEWcC9",
        "outputId": "2b211554-5334-49e2-cb81-84b4218538ca"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "============================================================\n",
            "         STRAIGHT-THROUGH ESTIMATOR\n",
            "============================================================\n",
            "\n",
            "Input:   [0.3 0.7 1.2 2.5]\n",
            "Rounded: [0. 1. 1. 2.]\n",
            "Gradient (straight-through): [0. 2. 2. 4.]\n",
            "\n",
            " Note: Round is non-differentiable, but we can still train!\n"
          ]
        }
      ],
      "source": [
        "# ============================================================================\n",
        "#                    STRAIGHT-THROUGH ESTIMATOR\n",
        "# ============================================================================\n",
        "\n",
        "print(\"=\"*60)\n",
        "print(\"         STRAIGHT-THROUGH ESTIMATOR\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "# The straight-through estimator is used for:\n",
        "# - Binary/discrete operations that are non-differentiable\n",
        "# - Quantization in neural networks\n",
        "\n",
        "@tf.custom_gradient\n",
        "def straight_through_round(x):\n",
        "    \"\"\"\n",
        "    Forward: round to nearest integer\n",
        "    Backward: pass gradient through unchanged (identity)\n",
        "    \"\"\"\n",
        "    def grad(dy):\n",
        "        return dy  # Straight-through: gradient = identity\n",
        "    return tf.round(x), grad\n",
        "\n",
        "@tf.custom_gradient\n",
        "def straight_through_sign(x):\n",
        "    \"\"\"\n",
        "    Forward: sign function (-1, 0, or 1)\n",
        "    Backward: gradient of hard tanh (1 if |x| <= 1, else 0)\n",
        "    \"\"\"\n",
        "    def grad(dy):\n",
        "        # Gradient is 1 where |x| <= 1, 0 elsewhere\n",
        "        return dy * tf.cast(tf.abs(x) <= 1, dy.dtype)\n",
        "    return tf.sign(x), grad\n",
        "\n",
        "# Test\n",
        "x = tf.Variable([0.3, 0.7, 1.2, 2.5])\n",
        "\n",
        "with tf.GradientTape() as tape:\n",
        "    y = straight_through_round(x)\n",
        "    loss = tf.reduce_sum(y ** 2)\n",
        "\n",
        "grad = tape.gradient(loss, x)\n",
        "\n",
        "print(f\"\\nInput:   {x.numpy()}\")\n",
        "print(f\"Rounded: {y.numpy()}\")\n",
        "print(f\"Gradient (straight-through): {grad.numpy()}\")\n",
        "print(f\"\\n Note: Round is non-differentiable, but we can still train!\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1Uwxn2-LWcC9",
        "outputId": "05f93a67-1e07-41be-f779-0f71a739b076"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "============================================================\n",
            "            GRADIENT ACCUMULATION\n",
            "============================================================\n",
            "\n",
            "Gradient Accumulation Pattern:\n",
            "  1. Compute gradients for mini-batch\n",
            "  2. Accumulate (sum or average) over N steps\n",
            "  3. Apply accumulated gradients once\n",
            "  4. Effective batch = mini_batch * N\n"
          ]
        }
      ],
      "source": [
        "# ============================================================================\n",
        "#                    GRADIENT ACCUMULATION\n",
        "# ============================================================================\n",
        "\n",
        "print(\"=\"*60)\n",
        "print(\"            GRADIENT ACCUMULATION\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "# Gradient accumulation is useful when:\n",
        "# - Batch size is too large for GPU memory\n",
        "# - You want effective larger batch sizes\n",
        "\n",
        "def train_with_accumulation(model, data, labels, batch_size, accumulation_steps, optimizer):\n",
        "    \"\"\"\n",
        "    Train with gradient accumulation.\n",
        "    Effective batch size = batch_size * accumulation_steps\n",
        "    \"\"\"\n",
        "    n_samples = len(data)\n",
        "    accumulated_gradients = [tf.zeros_like(v) for v in model.trainable_variables]\n",
        "\n",
        "    for step in range(accumulation_steps):\n",
        "        # Get mini-batch\n",
        "        start = (step * batch_size) % n_samples\n",
        "        end = start + batch_size\n",
        "        x_batch = data[start:end]\n",
        "        y_batch = labels[start:end]\n",
        "\n",
        "        with tf.GradientTape() as tape:\n",
        "            predictions = model(x_batch, training=True)\n",
        "            loss = tf.reduce_mean(keras.losses.mse(y_batch, predictions))\n",
        "\n",
        "        # Compute gradients\n",
        "        gradients = tape.gradient(loss, model.trainable_variables)\n",
        "\n",
        "        # Accumulate (average over steps)\n",
        "        accumulated_gradients = [\n",
        "            acc + grad / accumulation_steps\n",
        "            for acc, grad in zip(accumulated_gradients, gradients)\n",
        "        ]\n",
        "\n",
        "    # Apply accumulated gradients\n",
        "    optimizer.apply_gradients(zip(accumulated_gradients, model.trainable_variables))\n",
        "    return loss\n",
        "\n",
        "print(\"\\nGradient Accumulation Pattern:\")\n",
        "print(\"  1. Compute gradients for mini-batch\")\n",
        "print(\"  2. Accumulate (sum or average) over N steps\")\n",
        "print(\"  3. Apply accumulated gradients once\")\n",
        "print(\"  4. Effective batch = mini_batch * N\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3WCMnLwcWcC9"
      },
      "source": [
        "---\n",
        "\n",
        "# Part II: Building Operations from Scratch\n",
        "\n",
        "## Understanding Neural Network Primitives\n",
        "\n",
        "Before using Keras layers, let's understand what they do by building them ourselves."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PY-nVGvvWcC-",
        "outputId": "0fecf3b9-78a0-4020-e16b-c730fed07320"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "============================================================\n",
            "           CONVOLUTION FROM SCRATCH\n",
            "============================================================\n",
            "\n",
            "Input shape:  (1, 5, 5, 1)\n",
            "Kernel shape: (3, 3, 1, 2)\n",
            "Output shape: (1, 3, 3, 2)\n",
            "Matches tf.nn.conv2d: True\n"
          ]
        }
      ],
      "source": [
        "# ============================================================================\n",
        "#                    CONVOLUTION FROM SCRATCH\n",
        "# ============================================================================\n",
        "\n",
        "print(\"=\"*60)\n",
        "print(\"           CONVOLUTION FROM SCRATCH\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "def conv2d_naive(input_tensor, kernel, stride=1, padding='VALID'):\n",
        "    \"\"\"\n",
        "    Naive 2D convolution implementation.\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    input_tensor : tensor (batch, height, width, in_channels)\n",
        "    kernel : tensor (kernel_h, kernel_w, in_channels, out_channels)\n",
        "    stride : int\n",
        "    padding : 'VALID' or 'SAME'\n",
        "    \"\"\"\n",
        "    batch_size = tf.shape(input_tensor)[0]\n",
        "    in_h, in_w = input_tensor.shape[1], input_tensor.shape[2]\n",
        "    k_h, k_w = kernel.shape[0], kernel.shape[1]\n",
        "    out_channels = kernel.shape[3]\n",
        "\n",
        "    if padding == 'SAME':\n",
        "        pad_h = k_h // 2\n",
        "        pad_w = k_w // 2\n",
        "        input_tensor = tf.pad(input_tensor,\n",
        "                              [[0, 0], [pad_h, pad_h], [pad_w, pad_w], [0, 0]])\n",
        "        in_h += 2 * pad_h\n",
        "        in_w += 2 * pad_w\n",
        "\n",
        "    out_h = (in_h - k_h) // stride + 1\n",
        "    out_w = (in_w - k_w) // stride + 1\n",
        "\n",
        "    output = tf.TensorArray(dtype=tf.float32, size=out_h * out_w)\n",
        "    idx = 0\n",
        "\n",
        "    for i in range(out_h):\n",
        "        for j in range(out_w):\n",
        "            # Extract patch\n",
        "            h_start = i * stride\n",
        "            w_start = j * stride\n",
        "            patch = input_tensor[:, h_start:h_start+k_h, w_start:w_start+k_w, :]\n",
        "\n",
        "            # Convolve: sum over (h, w, in_channels), keep out_channels\n",
        "            # patch: (batch, k_h, k_w, in_c)\n",
        "            # kernel: (k_h, k_w, in_c, out_c)\n",
        "            conv = tf.einsum('bhwi,hwio->bo', patch, kernel)\n",
        "            output = output.write(idx, conv)\n",
        "            idx += 1\n",
        "\n",
        "    output = output.stack()  # (out_h*out_w, batch, out_c)\n",
        "    output = tf.transpose(output, [1, 0, 2])  # (batch, out_h*out_w, out_c)\n",
        "    output = tf.reshape(output, [batch_size, out_h, out_w, out_channels])\n",
        "\n",
        "    return output\n",
        "\n",
        "# Test our implementation\n",
        "x = tf.random.normal((1, 5, 5, 1))  # 1 image, 5x5, 1 channel\n",
        "kernel = tf.random.normal((3, 3, 1, 2))  # 3x3 kernel, 1->2 channels\n",
        "\n",
        "our_output = conv2d_naive(x, kernel, stride=1, padding='VALID')\n",
        "tf_output = tf.nn.conv2d(x, kernel, strides=1, padding='VALID')\n",
        "\n",
        "print(f\"\\nInput shape:  {x.shape}\")\n",
        "print(f\"Kernel shape: {kernel.shape}\")\n",
        "print(f\"Output shape: {our_output.shape}\")\n",
        "print(f\"Matches tf.nn.conv2d: {tf.reduce_all(tf.abs(our_output - tf_output) < 1e-5).numpy()}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ltjjl1lJWcC-",
        "outputId": "ff32ca96-ed25-48e7-89fa-6061d5aa692a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "============================================================\n",
            "           MAX POOLING FROM SCRATCH\n",
            "============================================================\n",
            "\n",
            "Input shape: (1, 4, 4, 2)\n",
            "Input (channel 0):\n",
            "[[ 1.  3.  5.  7.]\n",
            " [ 9. 11. 13. 15.]\n",
            " [17. 19. 21. 23.]\n",
            " [25. 27. 29. 31.]]\n",
            "\n",
            "Output shape: (1, 2, 2, 2)\n",
            "Output (channel 0):\n",
            "[[11. 15.]\n",
            " [27. 31.]]\n",
            "\n",
            "Matches tf.nn.max_pool2d: True\n"
          ]
        }
      ],
      "source": [
        "# ============================================================================\n",
        "#                    MAX POOLING FROM SCRATCH\n",
        "# ============================================================================\n",
        "\n",
        "print(\"=\"*60)\n",
        "print(\"           MAX POOLING FROM SCRATCH\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "def max_pool2d_naive(input_tensor, pool_size=2, stride=2):\n",
        "    \"\"\"\n",
        "    Naive max pooling implementation.\n",
        "\n",
        "    For each pool_size x pool_size window, take the maximum.\n",
        "    \"\"\"\n",
        "    batch_size = tf.shape(input_tensor)[0]\n",
        "    in_h, in_w, channels = input_tensor.shape[1:]\n",
        "\n",
        "    out_h = (in_h - pool_size) // stride + 1\n",
        "    out_w = (in_w - pool_size) // stride + 1\n",
        "\n",
        "    outputs = []\n",
        "\n",
        "    for i in range(out_h):\n",
        "        row = []\n",
        "        for j in range(out_w):\n",
        "            h_start = i * stride\n",
        "            w_start = j * stride\n",
        "            # Extract window\n",
        "            window = input_tensor[:, h_start:h_start+pool_size,\n",
        "                                  w_start:w_start+pool_size, :]\n",
        "            # Max over spatial dimensions\n",
        "            pooled = tf.reduce_max(window, axis=[1, 2])\n",
        "            row.append(pooled)\n",
        "        outputs.append(tf.stack(row, axis=1))\n",
        "\n",
        "    return tf.stack(outputs, axis=1)\n",
        "\n",
        "# Test\n",
        "x = tf.constant([[[[1., 2.], [3., 4.], [5., 6.], [7., 8.]],\n",
        "                  [[9., 10.], [11., 12.], [13., 14.], [15., 16.]],\n",
        "                  [[17., 18.], [19., 20.], [21., 22.], [23., 24.]],\n",
        "                  [[25., 26.], [27., 28.], [29., 30.], [31., 32.]]]])\n",
        "\n",
        "print(f\"\\nInput shape: {x.shape}\")\n",
        "print(f\"Input (channel 0):\")\n",
        "print(x[0, :, :, 0].numpy())\n",
        "\n",
        "our_pool = max_pool2d_naive(x, pool_size=2, stride=2)\n",
        "tf_pool = tf.nn.max_pool2d(x, ksize=2, strides=2, padding='VALID')\n",
        "\n",
        "print(f\"\\nOutput shape: {our_pool.shape}\")\n",
        "print(f\"Output (channel 0):\")\n",
        "print(our_pool[0, :, :, 0].numpy())\n",
        "print(f\"\\nMatches tf.nn.max_pool2d: {tf.reduce_all(our_pool == tf_pool).numpy()}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BeZuo5ITWcC-",
        "outputId": "cc561fbf-5eaa-426a-8e5d-b9302df89ff4"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "============================================================\n",
            "        BATCH NORMALIZATION FROM SCRATCH\n",
            "============================================================\n",
            "\n",
            "Input shape: (8, 4)\n",
            "Input mean per feature: [ 0.32574826 -0.22160122  0.37827352  0.3575499 ]\n",
            "Input std per feature:  [0.8102391  0.74234474 1.061351   1.0821929 ]\n",
            "\n",
            "Output (training) mean: [-2.9802322e-08  4.6566129e-08  7.4505806e-09 -2.2351742e-08]\n",
            "Output (training) std:  [0.9999923 0.9999908 0.9999955 0.9999958]\n",
            "\n",
            " After BatchNorm, each feature has ~0 mean and ~1 std!\n"
          ]
        }
      ],
      "source": [
        "# ============================================================================\n",
        "#                    BATCH NORMALIZATION FROM SCRATCH\n",
        "# ============================================================================\n",
        "\n",
        "print(\"=\"*60)\n",
        "print(\"        BATCH NORMALIZATION FROM SCRATCH\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "class BatchNormFromScratch:\n",
        "    \"\"\"\n",
        "    Batch Normalization implemented from scratch.\n",
        "\n",
        "    During training:\n",
        "        x_norm = (x - batch_mean) / sqrt(batch_var + epsilon)\n",
        "        y = gamma * x_norm + beta\n",
        "\n",
        "    During inference:\n",
        "        Use running mean and variance instead of batch statistics.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, num_features, epsilon=1e-5, momentum=0.1):\n",
        "        self.epsilon = epsilon\n",
        "        self.momentum = momentum\n",
        "\n",
        "        # Learnable parameters\n",
        "        self.gamma = tf.Variable(tf.ones(num_features), name='gamma')\n",
        "        self.beta = tf.Variable(tf.zeros(num_features), name='beta')\n",
        "\n",
        "        # Running statistics (not trainable)\n",
        "        self.running_mean = tf.Variable(tf.zeros(num_features), trainable=False)\n",
        "        self.running_var = tf.Variable(tf.ones(num_features), trainable=False)\n",
        "\n",
        "    def __call__(self, x, training=True):\n",
        "        if training:\n",
        "            # Compute batch statistics\n",
        "            batch_mean = tf.reduce_mean(x, axis=0)\n",
        "            batch_var = tf.math.reduce_variance(x, axis=0)\n",
        "\n",
        "            # Update running statistics\n",
        "            self.running_mean.assign(\n",
        "                (1 - self.momentum) * self.running_mean + self.momentum * batch_mean\n",
        "            )\n",
        "            self.running_var.assign(\n",
        "                (1 - self.momentum) * self.running_var + self.momentum * batch_var\n",
        "            )\n",
        "\n",
        "            mean, var = batch_mean, batch_var\n",
        "        else:\n",
        "            mean, var = self.running_mean, self.running_var\n",
        "\n",
        "        # Normalize\n",
        "        x_norm = (x - mean) / tf.sqrt(var + self.epsilon)\n",
        "\n",
        "        # Scale and shift\n",
        "        return self.gamma * x_norm + self.beta\n",
        "\n",
        "    @property\n",
        "    def trainable_variables(self):\n",
        "        return [self.gamma, self.beta]\n",
        "\n",
        "# Test\n",
        "bn = BatchNormFromScratch(num_features=4)\n",
        "x = tf.random.normal((8, 4))  # Batch of 8, 4 features\n",
        "\n",
        "y_train = bn(x, training=True)\n",
        "y_eval = bn(x, training=False)\n",
        "\n",
        "print(f\"\\nInput shape: {x.shape}\")\n",
        "print(f\"Input mean per feature: {tf.reduce_mean(x, axis=0).numpy()}\")\n",
        "print(f\"Input std per feature:  {tf.math.reduce_std(x, axis=0).numpy()}\")\n",
        "print(f\"\\nOutput (training) mean: {tf.reduce_mean(y_train, axis=0).numpy()}\")\n",
        "print(f\"Output (training) std:  {tf.math.reduce_std(y_train, axis=0).numpy()}\")\n",
        "print(f\"\\n After BatchNorm, each feature has ~0 mean and ~1 std!\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YR_REX3RWcC-",
        "outputId": "8f88571c-dbec-4478-ba47-170c125f407d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "============================================================\n",
            "        LAYER NORMALIZATION FROM SCRATCH\n",
            "============================================================\n",
            "\n",
            "Input shape: (2, 3, 4)\n",
            "\n",
            "For sample [0, 0, :]:\n",
            "  Input:  [ 0.65648675 -0.4130517   0.33997506 -1.0056272 ]\n",
            "  Output: [ 1.1744822  -0.47392502  0.6866642  -1.3872216 ]\n",
            "  Output mean: -0.000000\n",
            "  Output std:  1.0000\n",
            "\n",
            " Key difference:\n",
            "  BatchNorm: normalize across batch (for each feature)\n",
            "  LayerNorm: normalize across features (for each sample)\n"
          ]
        }
      ],
      "source": [
        "# ============================================================================\n",
        "#                    LAYER NORMALIZATION FROM SCRATCH\n",
        "# ============================================================================\n",
        "\n",
        "print(\"=\"*60)\n",
        "print(\"        LAYER NORMALIZATION FROM SCRATCH\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "class LayerNormFromScratch:\n",
        "    \"\"\"\n",
        "    Layer Normalization: Normalize across features (not batch).\n",
        "\n",
        "    Used in Transformers because:\n",
        "    - Works with any batch size (including 1)\n",
        "    - No running statistics needed\n",
        "    - Each sample normalized independently\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, normalized_shape, epsilon=1e-5):\n",
        "        self.epsilon = epsilon\n",
        "        self.normalized_shape = normalized_shape\n",
        "\n",
        "        # Learnable parameters\n",
        "        self.gamma = tf.Variable(tf.ones(normalized_shape), name='gamma')\n",
        "        self.beta = tf.Variable(tf.zeros(normalized_shape), name='beta')\n",
        "\n",
        "    def __call__(self, x):\n",
        "        # Compute statistics across last dimensions\n",
        "        mean = tf.reduce_mean(x, axis=-1, keepdims=True)\n",
        "        var = tf.math.reduce_variance(x, axis=-1, keepdims=True)\n",
        "\n",
        "        # Normalize\n",
        "        x_norm = (x - mean) / tf.sqrt(var + self.epsilon)\n",
        "\n",
        "        # Scale and shift\n",
        "        return self.gamma * x_norm + self.beta\n",
        "\n",
        "    @property\n",
        "    def trainable_variables(self):\n",
        "        return [self.gamma, self.beta]\n",
        "\n",
        "# Test\n",
        "ln = LayerNormFromScratch(normalized_shape=4)\n",
        "x = tf.random.normal((2, 3, 4))  # (batch, seq, features)\n",
        "\n",
        "y = ln(x)\n",
        "\n",
        "print(f\"\\nInput shape: {x.shape}\")\n",
        "print(f\"\\nFor sample [0, 0, :]:\")\n",
        "print(f\"  Input:  {x[0, 0, :].numpy()}\")\n",
        "print(f\"  Output: {y[0, 0, :].numpy()}\")\n",
        "print(f\"  Output mean: {tf.reduce_mean(y[0, 0, :]).numpy():.6f}\")\n",
        "print(f\"  Output std:  {tf.math.reduce_std(y[0, 0, :]).numpy():.4f}\")\n",
        "\n",
        "print(f\"\\n Key difference:\")\n",
        "print(f\"  BatchNorm: normalize across batch (for each feature)\")\n",
        "print(f\"  LayerNorm: normalize across features (for each sample)\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "D4Ok8o68WcC_",
        "outputId": "df224258-f4e9-4099-d105-c0b1f8bd5f38"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "============================================================\n",
            "            DROPOUT FROM SCRATCH\n",
            "============================================================\n",
            "Input: all ones, shape (2, 10)\n",
            "\n",
            "Dropout sample 1: [0. 2. 2. 0. 0. 2. 0. 2. 2. 0.]\n",
            "Dropout sample 2: [0. 2. 2. 2. 0. 0. 2. 2. 2. 0.]\n",
            "Dropout sample 3: [2. 0. 2. 2. 2. 0. 0. 2. 2. 2.]\n",
            "\n",
            "During inference (training=False):\n",
            "  Output: [1. 1. 1. 1. 1. 1. 1. 1. 1. 1.]\n",
            "\n",
            "Average over 1000 samples: 1.0045 (should be ~1.0)\n"
          ]
        }
      ],
      "source": [
        "# ============================================================================\n",
        "#                    DROPOUT FROM SCRATCH\n",
        "# ============================================================================\n",
        "\n",
        "print(\"=\"*60)\n",
        "print(\"            DROPOUT FROM SCRATCH\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "def dropout_from_scratch(x, rate=0.5, training=True):\n",
        "    \"\"\"\n",
        "    Dropout: Randomly zero out neurons during training.\n",
        "\n",
        "    Key insight: Scale by 1/(1-rate) during training so that\n",
        "    expected value remains the same during inference.\n",
        "    \"\"\"\n",
        "    if not training or rate == 0:\n",
        "        return x\n",
        "\n",
        "    # Create random mask\n",
        "    keep_prob = 1 - rate\n",
        "    mask = tf.cast(\n",
        "        tf.random.uniform(tf.shape(x)) < keep_prob,\n",
        "        dtype=x.dtype\n",
        "    )\n",
        "\n",
        "    # Apply mask and scale\n",
        "    return (x * mask) / keep_prob\n",
        "\n",
        "# Test\n",
        "x = tf.ones((2, 10))\n",
        "\n",
        "print(f\"Input: all ones, shape {x.shape}\")\n",
        "print(f\"\")\n",
        "\n",
        "# Multiple dropout samples\n",
        "for i in range(3):\n",
        "    dropped = dropout_from_scratch(x, rate=0.5, training=True)\n",
        "    print(f\"Dropout sample {i+1}: {dropped[0].numpy()}\")\n",
        "\n",
        "print(f\"\\nDuring inference (training=False):\")\n",
        "print(f\"  Output: {dropout_from_scratch(x, rate=0.5, training=False)[0].numpy()}\")\n",
        "\n",
        "# Verify expected value is preserved\n",
        "samples = tf.stack([dropout_from_scratch(x, rate=0.5) for _ in range(1000)])\n",
        "print(f\"\\nAverage over 1000 samples: {tf.reduce_mean(samples).numpy():.4f} (should be ~1.0)\")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "\n",
        "# Part III: Custom Layers Using Primitives\n",
        "\n",
        "## Building Layers with tf.Variable Only\n",
        "\n",
        "Before using Keras's layer system, let's build fully functional layers using only basic TensorFlow operations. This shows exactly what happens under the hood."
      ],
      "metadata": {
        "id": "4XqEZxTtWcC_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ============================================================================\n",
        "#                    DENSE LAYER FROM PRIMITIVES\n",
        "# ============================================================================\n",
        "\n",
        "print(\"=\"*60)\n",
        "print(\"          DENSE LAYER FROM PRIMITIVES\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "class DenseLayerPrimitive:\n",
        "    \"\"\"\n",
        "    Fully connected layer using only tf.Variable.\n",
        "\n",
        "    Mathematically: y = activation(x @ W + b)\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, in_features, out_features, activation=None, use_bias=True):\n",
        "        self.in_features = in_features\n",
        "        self.out_features = out_features\n",
        "        self.use_bias = use_bias\n",
        "\n",
        "        # Select activation\n",
        "        self.activation = {\n",
        "            None: lambda x: x,\n",
        "            'relu': tf.nn.relu,\n",
        "            'sigmoid': tf.nn.sigmoid,\n",
        "            'tanh': tf.nn.tanh,\n",
        "            'softmax': lambda x: tf.nn.softmax(x, axis=-1)\n",
        "        }.get(activation, activation)  # Allow passing functions directly\n",
        "\n",
        "        # He initialization for weights\n",
        "        stddev = np.sqrt(2.0 / in_features)\n",
        "        self.W = tf.Variable(\n",
        "            tf.random.normal((in_features, out_features), stddev=stddev),\n",
        "            trainable=True,\n",
        "            name='kernel'\n",
        "        )\n",
        "\n",
        "        if use_bias:\n",
        "            self.b = tf.Variable(\n",
        "                tf.zeros(out_features),\n",
        "                trainable=True,\n",
        "                name='bias'\n",
        "            )\n",
        "\n",
        "    def __call__(self, x):\n",
        "        \"\"\"Forward pass: y = activation(x @ W + b)\"\"\"\n",
        "        out = x @ self.W\n",
        "        if self.use_bias:\n",
        "            out = out + self.b\n",
        "        return self.activation(out)\n",
        "\n",
        "    @property\n",
        "    def trainable_variables(self):\n",
        "        if self.use_bias:\n",
        "            return [self.W, self.b]\n",
        "        return [self.W]\n",
        "\n",
        "    def __repr__(self):\n",
        "        return f\"DenseLayerPrimitive({self.in_features}, {self.out_features})\"\n",
        "\n",
        "# Test\n",
        "dense = DenseLayerPrimitive(4, 3, activation='relu')\n",
        "x = tf.random.normal((2, 4))\n",
        "y = dense(x)\n",
        "\n",
        "print(f\"\\nDenseLayerPrimitive(4, 3, activation='relu')\")\n",
        "print(f\"Input shape:  {x.shape}\")\n",
        "print(f\"Output shape: {y.shape}\")\n",
        "print(f\"Weight shape: {dense.W.shape}\")\n",
        "print(f\"Bias shape:   {dense.b.shape}\")\n",
        "print(f\"\\nOutput:\\n{y.numpy()}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gesc0NBxWcC_",
        "outputId": "b6715eba-e304-4b67-8b72-58af344a7930"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "============================================================\n",
            "          DENSE LAYER FROM PRIMITIVES\n",
            "============================================================\n",
            "\n",
            "DenseLayerPrimitive(4, 3, activation='relu')\n",
            "Input shape:  (2, 4)\n",
            "Output shape: (2, 3)\n",
            "Weight shape: (4, 3)\n",
            "Bias shape:   (3,)\n",
            "\n",
            "Output:\n",
            "[[0.         0.         0.46504724]\n",
            " [0.         0.         0.8249154 ]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ============================================================================\n",
        "#                    CONV2D LAYER FROM PRIMITIVES\n",
        "# ============================================================================\n",
        "\n",
        "print(\"=\"*60)\n",
        "print(\"          CONV2D LAYER FROM PRIMITIVES\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "class Conv2DLayerPrimitive:\n",
        "    \"\"\"\n",
        "    2D Convolutional layer using only tf.Variable and tf.nn.conv2d.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, in_channels, out_channels, kernel_size,\n",
        "                 stride=1, padding='SAME', activation=None, use_bias=True):\n",
        "        self.stride = stride\n",
        "        self.padding = padding\n",
        "        self.use_bias = use_bias\n",
        "\n",
        "        # Handle kernel_size as int or tuple\n",
        "        if isinstance(kernel_size, int):\n",
        "            kernel_size = (kernel_size, kernel_size)\n",
        "\n",
        "        # Select activation\n",
        "        self.activation = {\n",
        "            None: lambda x: x,\n",
        "            'relu': tf.nn.relu,\n",
        "            'sigmoid': tf.nn.sigmoid,\n",
        "            'tanh': tf.nn.tanh,\n",
        "        }.get(activation, activation)\n",
        "\n",
        "        # He initialization\n",
        "        fan_in = kernel_size[0] * kernel_size[1] * in_channels\n",
        "        stddev = np.sqrt(2.0 / fan_in)\n",
        "\n",
        "        # Kernel shape: (height, width, in_channels, out_channels)\n",
        "        self.kernel = tf.Variable(\n",
        "            tf.random.normal((kernel_size[0], kernel_size[1], in_channels, out_channels),\n",
        "                           stddev=stddev),\n",
        "            trainable=True,\n",
        "            name='kernel'\n",
        "        )\n",
        "\n",
        "        if use_bias:\n",
        "            self.bias = tf.Variable(\n",
        "                tf.zeros(out_channels),\n",
        "                trainable=True,\n",
        "                name='bias'\n",
        "            )\n",
        "\n",
        "    def __call__(self, x):\n",
        "        \"\"\"Forward pass using tf.nn.conv2d\"\"\"\n",
        "        out = tf.nn.conv2d(x, self.kernel, strides=self.stride, padding=self.padding)\n",
        "        if self.use_bias:\n",
        "            out = out + self.bias\n",
        "        return self.activation(out)\n",
        "\n",
        "    @property\n",
        "    def trainable_variables(self):\n",
        "        if self.use_bias:\n",
        "            return [self.kernel, self.bias]\n",
        "        return [self.kernel]\n",
        "\n",
        "# Test\n",
        "conv = Conv2DLayerPrimitive(3, 16, kernel_size=3, activation='relu')\n",
        "x = tf.random.normal((1, 28, 28, 3))  # 1 image, 28x28, 3 channels\n",
        "y = conv(x)\n",
        "\n",
        "print(f\"\\nConv2DLayerPrimitive(3, 16, kernel_size=3)\")\n",
        "print(f\"Input shape:  {x.shape}\")\n",
        "print(f\"Output shape: {y.shape}\")\n",
        "print(f\"Kernel shape: {conv.kernel.shape}\")\n",
        "print(f\"Parameters:   {np.prod(conv.kernel.shape) + conv.bias.shape[0]}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bSWj-SXwWcC_",
        "outputId": "4d0426cb-798c-4b89-f9fd-28b598540550"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "============================================================\n",
            "          CONV2D LAYER FROM PRIMITIVES\n",
            "============================================================\n",
            "\n",
            "Conv2DLayerPrimitive(3, 16, kernel_size=3)\n",
            "Input shape:  (1, 28, 28, 3)\n",
            "Output shape: (1, 28, 28, 16)\n",
            "Kernel shape: (3, 3, 3, 16)\n",
            "Parameters:   448\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ============================================================================\n",
        "#                    COMPLETE CNN FROM PRIMITIVES\n",
        "# ============================================================================\n",
        "\n",
        "print(\"=\"*60)\n",
        "print(\"          COMPLETE CNN FROM PRIMITIVES\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "class CNNFromPrimitives:\n",
        "    \"\"\"\n",
        "    A complete CNN built using only primitive layers.\n",
        "\n",
        "    Architecture:\n",
        "        Conv(3x3) -> ReLU -> MaxPool -> Conv(3x3) -> ReLU -> MaxPool -> Flatten -> Dense -> Dense\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, input_shape, num_classes):\n",
        "        self.input_shape = input_shape\n",
        "        in_channels = input_shape[-1]\n",
        "\n",
        "        # Convolutional layers\n",
        "        self.conv1 = Conv2DLayerPrimitive(in_channels, 32, kernel_size=3, activation='relu')\n",
        "        self.conv2 = Conv2DLayerPrimitive(32, 64, kernel_size=3, activation='relu')\n",
        "\n",
        "        # Calculate flattened size after convolutions and pooling\n",
        "        # With SAME padding and 2x2 pooling twice: H/4, W/4\n",
        "        h, w = input_shape[0] // 4, input_shape[1] // 4\n",
        "        flat_size = h * w * 64\n",
        "\n",
        "        # Dense layers\n",
        "        self.fc1 = DenseLayerPrimitive(flat_size, 128, activation='relu')\n",
        "        self.fc2 = DenseLayerPrimitive(128, num_classes, activation='softmax')\n",
        "\n",
        "        self.layers = [self.conv1, self.conv2, self.fc1, self.fc2]\n",
        "\n",
        "    def __call__(self, x, training=True):\n",
        "        # Conv block 1\n",
        "        x = self.conv1(x)\n",
        "        x = tf.nn.max_pool2d(x, ksize=2, strides=2, padding='SAME')\n",
        "\n",
        "        # Conv block 2\n",
        "        x = self.conv2(x)\n",
        "        x = tf.nn.max_pool2d(x, ksize=2, strides=2, padding='SAME')\n",
        "\n",
        "        # Flatten\n",
        "        x = tf.reshape(x, (tf.shape(x)[0], -1))\n",
        "\n",
        "        # Dense layers\n",
        "        x = self.fc1(x)\n",
        "        if training:\n",
        "            x = dropout_from_scratch(x, rate=0.5, training=True)\n",
        "        x = self.fc2(x)\n",
        "\n",
        "        return x\n",
        "\n",
        "    @property\n",
        "    def trainable_variables(self):\n",
        "        variables = []\n",
        "        for layer in self.layers:\n",
        "            variables.extend(layer.trainable_variables)\n",
        "        return variables\n",
        "\n",
        "    def summary(self):\n",
        "        print(\"\\n\" + \"=\"*50)\n",
        "        print(\"        CNN FROM PRIMITIVES - SUMMARY\")\n",
        "        print(\"=\"*50)\n",
        "        total = 0\n",
        "        for i, layer in enumerate(self.layers):\n",
        "            params = sum(np.prod(v.shape) for v in layer.trainable_variables)\n",
        "            total += params\n",
        "            print(f\"Layer {i+1}: {layer.__class__.__name__:20} | Params: {params:,}\")\n",
        "        print(\"-\"*50)\n",
        "        print(f\"Total trainable parameters: {total:,}\")\n",
        "\n",
        "# Create and test\n",
        "cnn = CNNFromPrimitives(input_shape=(28, 28, 1), num_classes=10)\n",
        "cnn.summary()\n",
        "\n",
        "# Test forward pass\n",
        "x = tf.random.normal((4, 28, 28, 1))\n",
        "y = cnn(x)\n",
        "print(f\"\\nInput shape:  {x.shape}\")\n",
        "print(f\"Output shape: {y.shape}\")\n",
        "print(f\"Output sum per sample: {tf.reduce_sum(y, axis=1).numpy()}  (should be ~1.0)\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BvCl_LZwWcC_",
        "outputId": "50736c3b-4e5c-4e2c-f9bb-eb8cc58ea664"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "============================================================\n",
            "          COMPLETE CNN FROM PRIMITIVES\n",
            "============================================================\n",
            "\n",
            "==================================================\n",
            "        CNN FROM PRIMITIVES - SUMMARY\n",
            "==================================================\n",
            "Layer 1: Conv2DLayerPrimitive | Params: 320\n",
            "Layer 2: Conv2DLayerPrimitive | Params: 18,496\n",
            "Layer 3: DenseLayerPrimitive  | Params: 401,536\n",
            "Layer 4: DenseLayerPrimitive  | Params: 1,290\n",
            "--------------------------------------------------\n",
            "Total trainable parameters: 421,642\n",
            "\n",
            "Input shape:  (4, 28, 28, 1)\n",
            "Output shape: (4, 10)\n",
            "Output sum per sample: [0.9999999 1.        1.        0.9999999]  (should be ~1.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "\n",
        "# Part IV: Custom Keras Layers\n",
        "\n",
        "## The Proper Way to Build Custom Layers\n",
        "\n",
        "Keras provides a clean API for custom layers with:\n",
        "- **`build()`**: Create weights when input shape is known\n",
        "- **`call()`**: Define the forward pass\n",
        "- **`get_config()`**: Enable serialization\n",
        "\n",
        "This gives you all the benefits of primitives PLUS:\n",
        "- Automatic weight tracking\n",
        "- Serialization/deserialization\n",
        "- Integration with `model.fit()`\n",
        "- Proper shape inference"
      ],
      "metadata": {
        "id": "CPUeDPKIWcC_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ============================================================================\n",
        "#                    CUSTOM KERAS LAYER: BASICS\n",
        "# ============================================================================\n",
        "\n",
        "print(\"=\"*60)\n",
        "print(\"          CUSTOM KERAS LAYER: BASICS\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "class CustomDenseLayer(keras.layers.Layer):\n",
        "    \"\"\"\n",
        "    Custom Dense layer demonstrating the Keras layer API.\n",
        "\n",
        "    Key methods:\n",
        "    - __init__: Store configuration (no weights yet!)\n",
        "    - build: Create weights when input shape is known\n",
        "    - call: Forward pass\n",
        "    - get_config: For serialization\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, units, activation=None, use_bias=True, **kwargs):\n",
        "        super().__init__(**kwargs)\n",
        "        self.units = units\n",
        "        self.activation = keras.activations.get(activation)\n",
        "        self.use_bias = use_bias\n",
        "\n",
        "    def build(self, input_shape):\n",
        "        \"\"\"\n",
        "        Create weights. Called automatically the first time the layer is used.\n",
        "\n",
        "        self.add_weight() creates a tf.Variable and registers it properly.\n",
        "        \"\"\"\n",
        "        self.kernel = self.add_weight(\n",
        "            name='kernel',\n",
        "            shape=(input_shape[-1], self.units),\n",
        "            initializer='glorot_uniform',  # Xavier initialization\n",
        "            trainable=True\n",
        "        )\n",
        "\n",
        "        if self.use_bias:\n",
        "            self.bias = self.add_weight(\n",
        "                name='bias',\n",
        "                shape=(self.units,),\n",
        "                initializer='zeros',\n",
        "                trainable=True\n",
        "            )\n",
        "\n",
        "        # Mark as built\n",
        "        super().build(input_shape)\n",
        "\n",
        "    def call(self, inputs):\n",
        "        \"\"\"Forward pass.\"\"\"\n",
        "        output = tf.matmul(inputs, self.kernel)\n",
        "        if self.use_bias:\n",
        "            output = output + self.bias\n",
        "        if self.activation is not None:\n",
        "            output = self.activation(output)\n",
        "        return output\n",
        "\n",
        "    def get_config(self):\n",
        "        \"\"\"Enable serialization.\"\"\"\n",
        "        config = super().get_config()\n",
        "        config.update({\n",
        "            'units': self.units,\n",
        "            'activation': keras.activations.serialize(self.activation),\n",
        "            'use_bias': self.use_bias\n",
        "        })\n",
        "        return config\n",
        "\n",
        "# Test custom layer\n",
        "custom_dense = CustomDenseLayer(32, activation='relu')\n",
        "x = tf.random.normal((4, 16))\n",
        "y = custom_dense(x)  # This triggers build()\n",
        "\n",
        "print(f\"\\nCustomDenseLayer(32, activation='relu')\")\n",
        "print(f\"Input shape:  {x.shape}\")\n",
        "print(f\"Output shape: {y.shape}\")\n",
        "print(f\"Kernel shape: {custom_dense.kernel.shape}\")\n",
        "print(f\"Trainable variables: {len(custom_dense.trainable_variables)}\")\n",
        "print(f\"\\nConfig: {custom_dense.get_config()}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TnKJDd05WcC_",
        "outputId": "016e8d21-60a0-4b25-ab79-fd4c4abfc37a"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "============================================================\n",
            "          CUSTOM KERAS LAYER: BASICS\n",
            "============================================================\n",
            "\n",
            "CustomDenseLayer(32, activation='relu')\n",
            "Input shape:  (4, 16)\n",
            "Output shape: (4, 32)\n",
            "Kernel shape: (16, 32)\n",
            "Trainable variables: 2\n",
            "\n",
            "Config: {'name': 'custom_dense_layer', 'trainable': True, 'dtype': {'module': 'keras', 'class_name': 'DTypePolicy', 'config': {'name': 'float32'}, 'registered_name': None}, 'units': 32, 'activation': 'relu', 'use_bias': True}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ============================================================================\n",
        "#                    CUSTOM LAYER: SELF-ATTENTION\n",
        "# ============================================================================\n",
        "\n",
        "print(\"=\"*60)\n",
        "print(\"         CUSTOM LAYER: SELF-ATTENTION\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "class SelfAttentionLayer(keras.layers.Layer):\n",
        "    \"\"\"\n",
        "    Self-Attention layer (simplified version of Transformer attention).\n",
        "\n",
        "    Attention(Q, K, V) = softmax(Q @ K^T / sqrt(d_k)) @ V\n",
        "\n",
        "    In self-attention, Q, K, V all come from the same input.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, embed_dim, num_heads=1, **kwargs):\n",
        "        super().__init__(**kwargs)\n",
        "        self.embed_dim = embed_dim\n",
        "        self.num_heads = num_heads\n",
        "        self.head_dim = embed_dim // num_heads\n",
        "\n",
        "        assert embed_dim % num_heads == 0, \"embed_dim must be divisible by num_heads\"\n",
        "\n",
        "    def build(self, input_shape):\n",
        "        # Linear projections for Q, K, V\n",
        "        self.W_q = self.add_weight(\n",
        "            name='W_q',\n",
        "            shape=(input_shape[-1], self.embed_dim),\n",
        "            initializer='glorot_uniform'\n",
        "        )\n",
        "        self.W_k = self.add_weight(\n",
        "            name='W_k',\n",
        "            shape=(input_shape[-1], self.embed_dim),\n",
        "            initializer='glorot_uniform'\n",
        "        )\n",
        "        self.W_v = self.add_weight(\n",
        "            name='W_v',\n",
        "            shape=(input_shape[-1], self.embed_dim),\n",
        "            initializer='glorot_uniform'\n",
        "        )\n",
        "        self.W_o = self.add_weight(\n",
        "            name='W_o',\n",
        "            shape=(self.embed_dim, self.embed_dim),\n",
        "            initializer='glorot_uniform'\n",
        "        )\n",
        "        super().build(input_shape)\n",
        "\n",
        "    def call(self, inputs, mask=None):\n",
        "        batch_size = tf.shape(inputs)[0]\n",
        "        seq_len = tf.shape(inputs)[1]\n",
        "\n",
        "        # Linear projections\n",
        "        Q = inputs @ self.W_q  # (batch, seq, embed)\n",
        "        K = inputs @ self.W_k\n",
        "        V = inputs @ self.W_v\n",
        "\n",
        "        # Reshape for multi-head attention\n",
        "        Q = tf.reshape(Q, (batch_size, seq_len, self.num_heads, self.head_dim))\n",
        "        K = tf.reshape(K, (batch_size, seq_len, self.num_heads, self.head_dim))\n",
        "        V = tf.reshape(V, (batch_size, seq_len, self.num_heads, self.head_dim))\n",
        "\n",
        "        # Transpose to (batch, heads, seq, head_dim)\n",
        "        Q = tf.transpose(Q, [0, 2, 1, 3])\n",
        "        K = tf.transpose(K, [0, 2, 1, 3])\n",
        "        V = tf.transpose(V, [0, 2, 1, 3])\n",
        "\n",
        "        # Attention scores: Q @ K^T / sqrt(d_k)\n",
        "        scale = tf.sqrt(tf.cast(self.head_dim, tf.float32))\n",
        "        scores = tf.matmul(Q, K, transpose_b=True) / scale  # (batch, heads, seq, seq)\n",
        "\n",
        "        # Apply mask if provided\n",
        "        if mask is not None:\n",
        "            scores += (1 - mask) * -1e9\n",
        "\n",
        "        # Softmax\n",
        "        attention_weights = tf.nn.softmax(scores, axis=-1)\n",
        "\n",
        "        # Apply attention to values\n",
        "        context = tf.matmul(attention_weights, V)  # (batch, heads, seq, head_dim)\n",
        "\n",
        "        # Reshape back\n",
        "        context = tf.transpose(context, [0, 2, 1, 3])  # (batch, seq, heads, head_dim)\n",
        "        context = tf.reshape(context, (batch_size, seq_len, self.embed_dim))\n",
        "\n",
        "        # Output projection\n",
        "        output = context @ self.W_o\n",
        "\n",
        "        return output, attention_weights\n",
        "\n",
        "    def get_config(self):\n",
        "        config = super().get_config()\n",
        "        config.update({\n",
        "            'embed_dim': self.embed_dim,\n",
        "            'num_heads': self.num_heads\n",
        "        })\n",
        "        return config\n",
        "\n",
        "# Test\n",
        "attention = SelfAttentionLayer(embed_dim=64, num_heads=4)\n",
        "x = tf.random.normal((2, 10, 64))  # (batch, seq_len, embed_dim)\n",
        "output, weights = attention(x)\n",
        "\n",
        "print(f\"\\nSelfAttentionLayer(embed_dim=64, num_heads=4)\")\n",
        "print(f\"Input shape:            {x.shape}\")\n",
        "print(f\"Output shape:           {output.shape}\")\n",
        "print(f\"Attention weights shape: {weights.shape}\")\n",
        "print(f\"Trainable parameters:   {sum(np.prod(v.shape) for v in attention.trainable_variables):,}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lfKOP_WWWcDA",
        "outputId": "a8f45bfe-f0a2-450d-e478-4f6bb9cad4d3"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "============================================================\n",
            "         CUSTOM LAYER: SELF-ATTENTION\n",
            "============================================================\n",
            "\n",
            "SelfAttentionLayer(embed_dim=64, num_heads=4)\n",
            "Input shape:            (2, 10, 64)\n",
            "Output shape:           (2, 10, 64)\n",
            "Attention weights shape: (2, 4, 10, 10)\n",
            "Trainable parameters:   16,384\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ============================================================================\n",
        "#                    CUSTOM LAYER: SPECTRAL NORMALIZATION\n",
        "# ============================================================================\n",
        "\n",
        "print(\"=\"*60)\n",
        "print(\"       CUSTOM LAYER: SPECTRAL NORMALIZATION\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "class SpectralNormalization(keras.layers.Wrapper):\n",
        "    \"\"\"\n",
        "    Spectral Normalization wrapper for layers.\n",
        "\n",
        "    Constrains the spectral norm (largest singular value) of the weight matrix.\n",
        "    Used in GANs to stabilize training.\n",
        "\n",
        "    W_normalized = W / sigma(W)\n",
        "    where sigma(W) is the largest singular value.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, layer, power_iterations=1, epsilon=1e-12, **kwargs):\n",
        "        super().__init__(layer, **kwargs)\n",
        "        self.power_iterations = power_iterations\n",
        "        self.epsilon = epsilon\n",
        "\n",
        "    def build(self, input_shape):\n",
        "        self.layer.build(input_shape)\n",
        "\n",
        "        # Get the weight matrix\n",
        "        self.w = self.layer.kernel\n",
        "        w_shape = self.w.shape.as_list()\n",
        "\n",
        "        # Flatten weight to 2D for SVD\n",
        "        self.w_flat_shape = (np.prod(w_shape[:-1]), w_shape[-1])\n",
        "\n",
        "        # Initialize u vector (for power iteration)\n",
        "        self.u = self.add_weight(\n",
        "            name='u',\n",
        "            shape=(1, w_shape[-1]),\n",
        "            initializer='random_normal',\n",
        "            trainable=False\n",
        "        )\n",
        "\n",
        "        super().build(input_shape)\n",
        "\n",
        "    def call(self, inputs, training=True):\n",
        "        # Power iteration to estimate largest singular value\n",
        "        w_flat = tf.reshape(self.w, self.w_flat_shape)\n",
        "\n",
        "        u = self.u\n",
        "        for _ in range(self.power_iterations):\n",
        "            # v = W^T u / ||W^T u||\n",
        "            v = tf.matmul(u, tf.transpose(w_flat))\n",
        "            v = v / (tf.norm(v, ord='euclidean') + self.epsilon)\n",
        "\n",
        "            # u = W v / ||W v||\n",
        "            u = tf.matmul(v, w_flat)\n",
        "            u = u / (tf.norm(u, ord='euclidean') + self.epsilon)\n",
        "\n",
        "        if training:\n",
        "            self.u.assign(u)\n",
        "\n",
        "        # Spectral norm: sigma = u^T W v\n",
        "        # Corrected calculation for row vectors u (1, out_features) and v (1, in_features)\n",
        "        # The formula should be v W u^T\n",
        "        sigma = tf.matmul(tf.matmul(v, w_flat), tf.transpose(u))\n",
        "\n",
        "        # Normalize weight\n",
        "        w_normalized = self.w / sigma[0, 0]\n",
        "\n",
        "        # Manually perform the forward pass of the wrapped layer using w_normalized.\n",
        "        # This assumes the wrapped layer is a Dense layer based on the test case.\n",
        "        output = tf.matmul(inputs, w_normalized)\n",
        "\n",
        "        # If the wrapped layer has a bias and uses it, add it.\n",
        "        if hasattr(self.layer, 'bias') and self.layer.use_bias:\n",
        "            output = output + self.layer.bias\n",
        "\n",
        "        # If the wrapped layer has an activation function, apply it.\n",
        "        if hasattr(self.layer, 'activation') and self.layer.activation is not None:\n",
        "            output = self.layer.activation(output)\n",
        "\n",
        "        return output\n",
        "\n",
        "# Test\n",
        "base_layer = layers.Dense(64)\n",
        "spectral_dense = SpectralNormalization(base_layer)\n",
        "x = tf.random.normal((4, 32))\n",
        "y = spectral_dense(x)\n",
        "\n",
        "print(f\"\\nSpectralNormalization(Dense(64))\")\n",
        "print(f\"Input shape:  {x.shape}\")\n",
        "print(f\"Output shape: {y.shape}\")\n",
        "print(f\"\\n Use case: Stabilize GAN discriminator training\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FFYq5xJbWcDA",
        "outputId": "fff8f033-4b10-4df8-bbf2-be569cd3fef1"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "============================================================\n",
            "       CUSTOM LAYER: SPECTRAL NORMALIZATION\n",
            "============================================================\n",
            "\n",
            "SpectralNormalization(Dense(64))\n",
            "Input shape:  (4, 32)\n",
            "Output shape: (4, 64)\n",
            "\n",
            " Use case: Stabilize GAN discriminator training\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "\n",
        "# Part V: Advanced Architectures\n",
        "\n",
        "## Building Modern Deep Learning Components\n",
        "\n",
        "Now let's build advanced architectural components used in state-of-the-art models:\n",
        "\n",
        "- **Residual Blocks** (ResNet)\n",
        "- **Squeeze-and-Excitation** (SENet)\n",
        "- **Transformer Encoder Block**\n",
        "- **Custom Normalization Layers**"
      ],
      "metadata": {
        "id": "biOHWrOFWcDA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ============================================================================\n",
        "#                    RESIDUAL BLOCK (ResNet Style)\n",
        "# ============================================================================\n",
        "\n",
        "print(\"=\"*60)\n",
        "print(\"           RESIDUAL BLOCK (ResNet)\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "class ResidualBlock(keras.layers.Layer):\n",
        "    \"\"\"\n",
        "    Residual Block: y = F(x) + x\n",
        "\n",
        "    The key insight: learning residual F(x) = y - x is easier than learning y directly.\n",
        "    This enables training very deep networks (100+ layers).\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, filters, kernel_size=3, stride=1, **kwargs):\n",
        "        super().__init__(**kwargs)\n",
        "        self.filters = filters\n",
        "        self.stride = stride\n",
        "\n",
        "        # Main path\n",
        "        self.conv1 = layers.Conv2D(filters, kernel_size, strides=stride,\n",
        "                                    padding='same', use_bias=False)\n",
        "        self.bn1 = layers.BatchNormalization()\n",
        "\n",
        "        self.conv2 = layers.Conv2D(filters, kernel_size, strides=1,\n",
        "                                    padding='same', use_bias=False)\n",
        "        self.bn2 = layers.BatchNormalization()\n",
        "\n",
        "        # Skip connection (identity or projection)\n",
        "        self.skip_conv = None\n",
        "        self.skip_bn = None\n",
        "\n",
        "    def build(self, input_shape):\n",
        "        # Need projection if dimensions change\n",
        "        if input_shape[-1] != self.filters or self.stride != 1:\n",
        "            self.skip_conv = layers.Conv2D(self.filters, 1, strides=self.stride,\n",
        "                                           padding='same', use_bias=False)\n",
        "            self.skip_bn = layers.BatchNormalization()\n",
        "        super().build(input_shape)\n",
        "\n",
        "    def call(self, inputs, training=False):\n",
        "        # Main path\n",
        "        x = self.conv1(inputs)\n",
        "        x = self.bn1(x, training=training)\n",
        "        x = tf.nn.relu(x)\n",
        "\n",
        "        x = self.conv2(x)\n",
        "        x = self.bn2(x, training=training)\n",
        "\n",
        "        # Skip connection\n",
        "        if self.skip_conv is not None:\n",
        "            skip = self.skip_conv(inputs)\n",
        "            skip = self.skip_bn(skip, training=training)\n",
        "        else:\n",
        "            skip = inputs\n",
        "\n",
        "        # Add and activate\n",
        "        return tf.nn.relu(x + skip)\n",
        "\n",
        "# Test\n",
        "res_block = ResidualBlock(64, stride=1)\n",
        "x = tf.random.normal((2, 32, 32, 64))\n",
        "y = res_block(x)\n",
        "\n",
        "print(f\"\\nResidualBlock(64)\")\n",
        "print(f\"Input shape:  {x.shape}\")\n",
        "print(f\"Output shape: {y.shape}\")\n",
        "\n",
        "# With downsampling\n",
        "res_block_down = ResidualBlock(128, stride=2)\n",
        "y_down = res_block_down(x)\n",
        "print(f\"\\nResidualBlock(128, stride=2)\")\n",
        "print(f\"Output shape: {y_down.shape}  (spatial dims halved, channels doubled)\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zqnNTvJTWcDA",
        "outputId": "75f6d364-a1b6-431a-f6ff-985d85b842e7"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "============================================================\n",
            "           RESIDUAL BLOCK (ResNet)\n",
            "============================================================\n",
            "\n",
            "ResidualBlock(64)\n",
            "Input shape:  (2, 32, 32, 64)\n",
            "Output shape: (2, 32, 32, 64)\n",
            "\n",
            "ResidualBlock(128, stride=2)\n",
            "Output shape: (2, 16, 16, 128)  (spatial dims halved, channels doubled)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ============================================================================\n",
        "#                    SQUEEZE-AND-EXCITATION BLOCK\n",
        "# ============================================================================\n",
        "\n",
        "print(\"=\"*60)\n",
        "print(\"         SQUEEZE-AND-EXCITATION BLOCK\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "class SqueezeExcitationBlock(keras.layers.Layer):\n",
        "    \"\"\"\n",
        "    Squeeze-and-Excitation (SE) Block.\n",
        "\n",
        "    Learns channel-wise attention weights:\n",
        "    1. Squeeze: Global average pooling (H,W,C) -> (1,1,C)\n",
        "    2. Excitation: FC -> ReLU -> FC -> Sigmoid\n",
        "    3. Scale: Multiply input by attention weights\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, reduction_ratio=16, **kwargs):\n",
        "        super().__init__(**kwargs)\n",
        "        self.reduction_ratio = reduction_ratio\n",
        "\n",
        "    def build(self, input_shape):\n",
        "        channels = input_shape[-1]\n",
        "        reduced_channels = max(channels // self.reduction_ratio, 1)\n",
        "\n",
        "        self.fc1 = layers.Dense(reduced_channels, activation='relu')\n",
        "        self.fc2 = layers.Dense(channels, activation='sigmoid')\n",
        "\n",
        "        super().build(input_shape)\n",
        "\n",
        "    def call(self, inputs):\n",
        "        # Squeeze: Global Average Pooling\n",
        "        squeezed = tf.reduce_mean(inputs, axis=[1, 2], keepdims=True)  # (B, 1, 1, C)\n",
        "\n",
        "        # Excitation\n",
        "        x = tf.reshape(squeezed, (tf.shape(inputs)[0], -1))  # (B, C)\n",
        "        x = self.fc1(x)\n",
        "        x = self.fc2(x)\n",
        "        x = tf.reshape(x, (tf.shape(inputs)[0], 1, 1, -1))  # (B, 1, 1, C)\n",
        "\n",
        "        # Scale\n",
        "        return inputs * x\n",
        "\n",
        "# Test\n",
        "se_block = SqueezeExcitationBlock(reduction_ratio=16)\n",
        "x = tf.random.normal((2, 28, 28, 64))\n",
        "y = se_block(x)\n",
        "\n",
        "print(f\"\\nSqueezeExcitationBlock(reduction_ratio=16)\")\n",
        "print(f\"Input shape:  {x.shape}\")\n",
        "print(f\"Output shape: {y.shape}\")\n",
        "print(f\"\\n SE learns which channels are important for the task\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kcm-_VhvWcDA",
        "outputId": "07d2ff10-3679-49ad-d244-43a352fab899"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "============================================================\n",
            "         SQUEEZE-AND-EXCITATION BLOCK\n",
            "============================================================\n",
            "\n",
            "SqueezeExcitationBlock(reduction_ratio=16)\n",
            "Input shape:  (2, 28, 28, 64)\n",
            "Output shape: (2, 28, 28, 64)\n",
            "\n",
            " SE learns which channels are important for the task\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ============================================================================\n",
        "#                    TRANSFORMER ENCODER BLOCK\n",
        "# ============================================================================\n",
        "\n",
        "print(\"=\"*60)\n",
        "print(\"          TRANSFORMER ENCODER BLOCK\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "class TransformerEncoderBlock(keras.layers.Layer):\n",
        "    \"\"\"\n",
        "    Transformer Encoder Block.\n",
        "\n",
        "    Architecture:\n",
        "        x -> LayerNorm -> MultiHeadAttention -> + (residual) ->\n",
        "             LayerNorm -> FeedForward -> + (residual) -> output\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, embed_dim, num_heads, ff_dim, dropout_rate=0.1, **kwargs):\n",
        "        super().__init__(**kwargs)\n",
        "        self.embed_dim = embed_dim\n",
        "        self.num_heads = num_heads\n",
        "        self.ff_dim = ff_dim\n",
        "\n",
        "        # Multi-head attention\n",
        "        self.attention = layers.MultiHeadAttention(\n",
        "            num_heads=num_heads,\n",
        "            key_dim=embed_dim // num_heads\n",
        "        )\n",
        "\n",
        "        # Feed-forward network\n",
        "        self.ffn = Sequential([\n",
        "            layers.Dense(ff_dim, activation='gelu'),\n",
        "            layers.Dropout(dropout_rate),\n",
        "            layers.Dense(embed_dim),\n",
        "            layers.Dropout(dropout_rate)\n",
        "        ])\n",
        "\n",
        "        # Layer normalization\n",
        "        self.layernorm1 = layers.LayerNormalization(epsilon=1e-6)\n",
        "        self.layernorm2 = layers.LayerNormalization(epsilon=1e-6)\n",
        "\n",
        "        # Dropout\n",
        "        self.dropout1 = layers.Dropout(dropout_rate)\n",
        "\n",
        "    def call(self, inputs, training=False, mask=None):\n",
        "        # Pre-norm architecture (more stable)\n",
        "        # Attention block\n",
        "        x = self.layernorm1(inputs)\n",
        "        attn_output = self.attention(x, x, attention_mask=mask, training=training)\n",
        "        attn_output = self.dropout1(attn_output, training=training)\n",
        "        x = inputs + attn_output  # Residual connection\n",
        "\n",
        "        # Feed-forward block\n",
        "        ffn_input = self.layernorm2(x)\n",
        "        ffn_output = self.ffn(ffn_input, training=training)\n",
        "\n",
        "        return x + ffn_output  # Residual connection\n",
        "\n",
        "# Test\n",
        "transformer_block = TransformerEncoderBlock(\n",
        "    embed_dim=64,\n",
        "    num_heads=4,\n",
        "    ff_dim=256,\n",
        "    dropout_rate=0.1\n",
        ")\n",
        "\n",
        "x = tf.random.normal((2, 20, 64))  # (batch, seq_len, embed_dim)\n",
        "y = transformer_block(x)\n",
        "\n",
        "print(f\"\\nTransformerEncoderBlock(embed_dim=64, num_heads=4, ff_dim=256)\")\n",
        "print(f\"Input shape:  {x.shape}\")\n",
        "print(f\"Output shape: {y.shape}\")\n",
        "print(f\"Parameters:   {sum(np.prod(v.shape) for v in transformer_block.trainable_variables):,}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1I-38YHnWcDA",
        "outputId": "c2b50ceb-61c5-4921-de9b-574440458403"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "============================================================\n",
            "          TRANSFORMER ENCODER BLOCK\n",
            "============================================================\n",
            "\n",
            "TransformerEncoderBlock(embed_dim=64, num_heads=4, ff_dim=256)\n",
            "Input shape:  (2, 20, 64)\n",
            "Output shape: (2, 20, 64)\n",
            "Parameters:   49,984\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "\n",
        "# Part VI: Custom Training Loops\n",
        "\n",
        "## Full Control with GradientTape\n",
        "\n",
        "While `model.fit()` is convenient, custom training loops give you:\n",
        "\n",
        "- **Complete control** over the training process\n",
        "- **Custom metrics** and logging\n",
        "- **Complex training schemes** (GANs, reinforcement learning)\n",
        "- **Gradient manipulation** (clipping, accumulation)\n",
        "- **Multi-GPU/TPU** strategies"
      ],
      "metadata": {
        "id": "z13eQIQpWcDA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ============================================================================\n",
        "#                    BASIC CUSTOM TRAINING LOOP\n",
        "# ============================================================================\n",
        "\n",
        "print(\"=\"*60)\n",
        "print(\"          BASIC CUSTOM TRAINING LOOP\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "def custom_training_loop(model, train_data, val_data, epochs, learning_rate=0.001):\n",
        "    \"\"\"\n",
        "    Complete custom training loop with GradientTape.\n",
        "    \"\"\"\n",
        "    optimizer = keras.optimizers.Adam(learning_rate)\n",
        "    loss_fn = keras.losses.SparseCategoricalCrossentropy()\n",
        "\n",
        "    # Metrics\n",
        "    train_loss = keras.metrics.Mean()\n",
        "    train_acc = keras.metrics.SparseCategoricalAccuracy()\n",
        "    val_loss = keras.metrics.Mean()\n",
        "    val_acc = keras.metrics.SparseCategoricalAccuracy()\n",
        "\n",
        "    history = {'train_loss': [], 'train_acc': [], 'val_loss': [], 'val_acc': []}\n",
        "\n",
        "    for epoch in range(epochs):\n",
        "        # Reset metrics\n",
        "        train_loss.reset_state()\n",
        "        train_acc.reset_state()\n",
        "\n",
        "        # Training loop\n",
        "        for x_batch, y_batch in train_data:\n",
        "            with tf.GradientTape() as tape:\n",
        "                # Forward pass\n",
        "                predictions = model(x_batch, training=True)\n",
        "                loss = loss_fn(y_batch, predictions)\n",
        "\n",
        "            # Backward pass\n",
        "            gradients = tape.gradient(loss, model.trainable_variables)\n",
        "\n",
        "            # Update weights\n",
        "            optimizer.apply_gradients(zip(gradients, model.trainable_variables))\n",
        "\n",
        "            # Update metrics\n",
        "            train_loss.update_state(loss)\n",
        "            train_acc.update_state(y_batch, predictions)\n",
        "\n",
        "        # Validation loop\n",
        "        val_loss.reset_state()\n",
        "        val_acc.reset_state()\n",
        "\n",
        "        for x_batch, y_batch in val_data:\n",
        "            predictions = model(x_batch, training=False)\n",
        "            loss = loss_fn(y_batch, predictions)\n",
        "            val_loss.update_state(loss)\n",
        "            val_acc.update_state(y_batch, predictions)\n",
        "\n",
        "        # Record history\n",
        "        history['train_loss'].append(train_loss.result().numpy())\n",
        "        history['train_acc'].append(train_acc.result().numpy())\n",
        "        history['val_loss'].append(val_loss.result().numpy())\n",
        "        history['val_acc'].append(val_acc.result().numpy())\n",
        "\n",
        "        # Print progress\n",
        "        print(f\"Epoch {epoch+1}/{epochs} | \"\n",
        "              f\"Train Loss: {train_loss.result():.4f} | \"\n",
        "              f\"Train Acc: {train_acc.result():.4f} | \"\n",
        "              f\"Val Loss: {val_loss.result():.4f} | \"\n",
        "              f\"Val Acc: {val_acc.result():.4f}\")\n",
        "\n",
        "    return history\n",
        "\n",
        "print(\"\\nCustom training loop template created!\")\n",
        "print(\"Key components:\")\n",
        "print(\"  1. GradientTape for computing gradients\")\n",
        "print(\"  2. optimizer.apply_gradients() for weight updates\")\n",
        "print(\"  3. Metrics for tracking performance\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0S7sZ5woWcDA",
        "outputId": "2cfb7e25-4322-404a-fe65-60b51432a413"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "============================================================\n",
            "          BASIC CUSTOM TRAINING LOOP\n",
            "============================================================\n",
            "\n",
            "Custom training loop template created!\n",
            "Key components:\n",
            "  1. GradientTape for computing gradients\n",
            "  2. optimizer.apply_gradients() for weight updates\n",
            "  3. Metrics for tracking performance\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ============================================================================\n",
        "#                    TRAINING WITH GRADIENT CLIPPING\n",
        "# ============================================================================\n",
        "\n",
        "print(\"=\"*60)\n",
        "print(\"        TRAINING WITH GRADIENT CLIPPING\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "@tf.function\n",
        "def train_step_with_clipping(model, x, y, optimizer, loss_fn, clip_norm=1.0):\n",
        "    \"\"\"\n",
        "    Single training step with gradient clipping.\n",
        "\n",
        "    Gradient clipping prevents exploding gradients in deep networks.\n",
        "    \"\"\"\n",
        "    with tf.GradientTape() as tape:\n",
        "        predictions = model(x, training=True)\n",
        "        loss = loss_fn(y, predictions)\n",
        "\n",
        "    # Compute gradients\n",
        "    gradients = tape.gradient(loss, model.trainable_variables)\n",
        "\n",
        "    # Clip gradients by global norm\n",
        "    gradients, global_norm = tf.clip_by_global_norm(gradients, clip_norm)\n",
        "\n",
        "    # Apply clipped gradients\n",
        "    optimizer.apply_gradients(zip(gradients, model.trainable_variables))\n",
        "\n",
        "    return loss, global_norm\n",
        "\n",
        "print(\"Gradient Clipping Options:\")\n",
        "print(\"  - tf.clip_by_value(g, -clip, clip)  : Clip element-wise\")\n",
        "print(\"  - tf.clip_by_norm(g, clip)          : Clip each tensor by L2 norm\")\n",
        "print(\"  - tf.clip_by_global_norm(grads, clip): Clip all gradients together\")\n",
        "print(\"\\nGlobal norm is most common - maintains gradient direction!\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nt4KslvnWcDA",
        "outputId": "3c78b1f6-9236-4300-b601-1f7d344b4c08"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "============================================================\n",
            "        TRAINING WITH GRADIENT CLIPPING\n",
            "============================================================\n",
            "Gradient Clipping Options:\n",
            "  - tf.clip_by_value(g, -clip, clip)  : Clip element-wise\n",
            "  - tf.clip_by_norm(g, clip)          : Clip each tensor by L2 norm\n",
            "  - tf.clip_by_global_norm(grads, clip): Clip all gradients together\n",
            "\n",
            "Global norm is most common - maintains gradient direction!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ============================================================================\n",
        "#                    CUSTOM MODEL WITH train_step()\n",
        "# ============================================================================\n",
        "\n",
        "print(\"=\"*60)\n",
        "print(\"        CUSTOM MODEL WITH train_step()\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "class CustomTrainableModel(keras.Model):\n",
        "    \"\"\"\n",
        "    Model with custom training logic built-in.\n",
        "\n",
        "    Override train_step() to customize what happens in model.fit().\n",
        "    This is the best of both worlds:\n",
        "    - Custom training logic\n",
        "    - Still use model.fit() with callbacks, validation, etc.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, units_list, num_classes, **kwargs):\n",
        "        super().__init__(**kwargs)\n",
        "\n",
        "        self.dense_layers = []\n",
        "        for units in units_list:\n",
        "            self.dense_layers.append(layers.Dense(units, activation='relu'))\n",
        "            self.dense_layers.append(layers.Dropout(0.2))\n",
        "\n",
        "        self.output_layer = layers.Dense(num_classes, activation='softmax')\n",
        "\n",
        "        # Custom metrics\n",
        "        self.loss_tracker = keras.metrics.Mean(name='loss')\n",
        "        self.acc_tracker = keras.metrics.SparseCategoricalAccuracy(name='accuracy')\n",
        "\n",
        "    def call(self, inputs, training=False):\n",
        "        x = inputs\n",
        "        for layer in self.dense_layers:\n",
        "            x = layer(x, training=training)\n",
        "        return self.output_layer(x)\n",
        "\n",
        "    def train_step(self, data):\n",
        "        \"\"\"\n",
        "        Custom training step.\n",
        "        Called by model.fit() for each batch.\n",
        "        \"\"\"\n",
        "        x, y = data\n",
        "\n",
        "        with tf.GradientTape() as tape:\n",
        "            y_pred = self(x, training=True)\n",
        "            loss = keras.losses.sparse_categorical_crossentropy(y, y_pred)\n",
        "            loss = tf.reduce_mean(loss)\n",
        "\n",
        "        # Compute and apply gradients\n",
        "        gradients = tape.gradient(loss, self.trainable_variables)\n",
        "        self.optimizer.apply_gradients(zip(gradients, self.trainable_variables))\n",
        "\n",
        "        # Update metrics\n",
        "        self.loss_tracker.update_state(loss)\n",
        "        self.acc_tracker.update_state(y, y_pred)\n",
        "\n",
        "        return {\n",
        "            'loss': self.loss_tracker.result(),\n",
        "            'accuracy': self.acc_tracker.result()\n",
        "        }\n",
        "\n",
        "    def test_step(self, data):\n",
        "        \"\"\"Custom evaluation step.\"\"\"\n",
        "        x, y = data\n",
        "        y_pred = self(x, training=False)\n",
        "        loss = keras.losses.sparse_categorical_crossentropy(y, y_pred)\n",
        "\n",
        "        self.loss_tracker.update_state(tf.reduce_mean(loss))\n",
        "        self.acc_tracker.update_state(y, y_pred)\n",
        "\n",
        "        return {\n",
        "            'loss': self.loss_tracker.result(),\n",
        "            'accuracy': self.acc_tracker.result()\n",
        "        }\n",
        "\n",
        "    @property\n",
        "    def metrics(self):\n",
        "        return [self.loss_tracker, self.acc_tracker]\n",
        "\n",
        "# Test\n",
        "custom_model = CustomTrainableModel([64, 32], num_classes=10)\n",
        "custom_model.compile(optimizer='adam')\n",
        "\n",
        "print(\"\\nCustom model with train_step() created!\")\n",
        "print(\"Now model.fit() uses our custom training logic\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4ocCaxVeWcDA",
        "outputId": "4a36cdc9-141c-43dc-b51c-d14857809ed4"
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "============================================================\n",
            "        CUSTOM MODEL WITH train_step()\n",
            "============================================================\n",
            "\n",
            "Custom model with train_step() created!\n",
            "Now model.fit() uses our custom training logic\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "\n",
        "# Part VII: Practical Demos\n",
        "\n",
        "## Putting It All Together\n",
        "\n",
        "Let's combine everything we've learned in real examples."
      ],
      "metadata": {
        "id": "S6cCw5HyWcDA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ============================================================================\n",
        "#              DEMO 1: CUSTOM RESNET FOR DIGIT CLASSIFICATION\n",
        "# ============================================================================\n",
        "\n",
        "print(\"=\"*60)\n",
        "print(\"     DEMO: CUSTOM RESNET FOR DIGIT CLASSIFICATION\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "from sklearn.datasets import load_digits\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "# Load digits dataset\n",
        "digits = load_digits()\n",
        "X, y = digits.data, digits.target\n",
        "\n",
        "# Preprocess\n",
        "scaler = StandardScaler()\n",
        "X = scaler.fit_transform(X)\n",
        "\n",
        "# Reshape for CNN: (samples, 8, 8, 1)\n",
        "X = X.reshape(-1, 8, 8, 1).astype(np.float32)\n",
        "\n",
        "# Split\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "print(f\"Training samples: {X_train.shape[0]}\")\n",
        "print(f\"Test samples:     {X_test.shape[0]}\")\n",
        "print(f\"Image shape:      {X_train.shape[1:]}\")\n",
        "\n",
        "# Build custom ResNet model\n",
        "class MiniResNet(keras.Model):\n",
        "    \"\"\"Mini ResNet with custom residual blocks.\"\"\"\n",
        "\n",
        "    def __init__(self, num_classes=10):\n",
        "        super().__init__()\n",
        "\n",
        "        # Initial convolution\n",
        "        self.conv1 = layers.Conv2D(32, 3, padding='same', activation='relu')\n",
        "\n",
        "        # Residual blocks\n",
        "        self.res_block1 = ResidualBlock(32)\n",
        "        self.res_block2 = ResidualBlock(64, stride=2)\n",
        "\n",
        "        # SE block for channel attention\n",
        "        self.se_block = SqueezeExcitationBlock(reduction_ratio=8)\n",
        "\n",
        "        # Classification head\n",
        "        self.global_pool = layers.GlobalAveragePooling2D()\n",
        "        self.dropout = layers.Dropout(0.3)\n",
        "        self.dense = layers.Dense(num_classes, activation='softmax')\n",
        "\n",
        "    def call(self, x, training=False):\n",
        "        x = self.conv1(x)\n",
        "        x = self.res_block1(x, training=training)\n",
        "        x = self.res_block2(x, training=training)\n",
        "        x = self.se_block(x)\n",
        "        x = self.global_pool(x)\n",
        "        x = self.dropout(x, training=training)\n",
        "        return self.dense(x)\n",
        "\n",
        "# Create and compile\n",
        "tf.random.set_seed(42)\n",
        "resnet_model = MiniResNet(num_classes=10)\n",
        "\n",
        "resnet_model.compile(\n",
        "    optimizer=keras.optimizers.Adam(learning_rate=0.001),\n",
        "    loss='sparse_categorical_crossentropy',\n",
        "    metrics=['accuracy']\n",
        ")\n",
        "\n",
        "# Build model by calling it\n",
        "_ = resnet_model(X_train[:1])\n",
        "resnet_model.summary()\n",
        "\n",
        "# Train\n",
        "print(\"\\nTraining MiniResNet...\")\n",
        "history = resnet_model.fit(\n",
        "    X_train, y_train,\n",
        "    validation_data=(X_test, y_test),\n",
        "    epochs=30,\n",
        "    batch_size=32,\n",
        "    verbose=1\n",
        ")\n",
        "\n",
        "# Evaluate\n",
        "test_loss, test_acc = resnet_model.evaluate(X_test, y_test, verbose=0)\n",
        "print(f\"\\nTest Accuracy: {test_acc*100:.2f}%\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "hJng8MQqWcDA",
        "outputId": "a6cd3895-9f79-418e-8361-35d8f41be51a"
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "============================================================\n",
            "     DEMO: CUSTOM RESNET FOR DIGIT CLASSIFICATION\n",
            "============================================================\n",
            "Training samples: 1437\n",
            "Test samples:     360\n",
            "Image shape:      (8, 8, 1)\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1mModel: \"mini_res_net\"\u001b[0m\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"mini_res_net\"</span>\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\n",
              "\u001b[1m \u001b[0m\u001b[1mLayer (type)                   \u001b[0m\u001b[1m \u001b[0m\u001b[1m \u001b[0m\u001b[1mOutput Shape          \u001b[0m\u001b[1m \u001b[0m\u001b[1m \u001b[0m\u001b[1m      Param #\u001b[0m\u001b[1m \u001b[0m\n",
              "\n",
              " conv2d_5 (\u001b[38;5;33mConv2D\u001b[0m)                (\u001b[38;5;34m1\u001b[0m, \u001b[38;5;34m8\u001b[0m, \u001b[38;5;34m8\u001b[0m, \u001b[38;5;34m32\u001b[0m)                     \u001b[38;5;34m320\u001b[0m \n",
              "\n",
              " residual_block_2                 ?                              \u001b[38;5;34m18,688\u001b[0m \n",
              " (\u001b[38;5;33mResidualBlock\u001b[0m)                                                        \n",
              "\n",
              " residual_block_3                 ?                              \u001b[38;5;34m58,112\u001b[0m \n",
              " (\u001b[38;5;33mResidualBlock\u001b[0m)                                                        \n",
              "\n",
              " squeeze_excitation_block_1       ?                               \u001b[38;5;34m1,096\u001b[0m \n",
              " (\u001b[38;5;33mSqueezeExcitationBlock\u001b[0m)                                               \n",
              "\n",
              " global_average_pooling2d         ?                                   \u001b[38;5;34m0\u001b[0m \n",
              " (\u001b[38;5;33mGlobalAveragePooling2D\u001b[0m)                                               \n",
              "\n",
              " dropout_6 (\u001b[38;5;33mDropout\u001b[0m)              ?                                   \u001b[38;5;34m0\u001b[0m \n",
              "\n",
              " dense_8 (\u001b[38;5;33mDense\u001b[0m)                  (\u001b[38;5;34m1\u001b[0m, \u001b[38;5;34m10\u001b[0m)                           \u001b[38;5;34m650\u001b[0m \n",
              "\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">\n",
              "<span style=\"font-weight: bold\"> Layer (type)                    </span><span style=\"font-weight: bold\"> Output Shape           </span><span style=\"font-weight: bold\">       Param # </span>\n",
              "\n",
              " conv2d_5 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)                (<span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">8</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">8</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)                     <span style=\"color: #00af00; text-decoration-color: #00af00\">320</span> \n",
              "\n",
              " residual_block_2                 ?                              <span style=\"color: #00af00; text-decoration-color: #00af00\">18,688</span> \n",
              " (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">ResidualBlock</span>)                                                        \n",
              "\n",
              " residual_block_3                 ?                              <span style=\"color: #00af00; text-decoration-color: #00af00\">58,112</span> \n",
              " (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">ResidualBlock</span>)                                                        \n",
              "\n",
              " squeeze_excitation_block_1       ?                               <span style=\"color: #00af00; text-decoration-color: #00af00\">1,096</span> \n",
              " (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">SqueezeExcitationBlock</span>)                                               \n",
              "\n",
              " global_average_pooling2d         ?                                   <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> \n",
              " (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">GlobalAveragePooling2D</span>)                                               \n",
              "\n",
              " dropout_6 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)              ?                                   <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> \n",
              "\n",
              " dense_8 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                  (<span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">10</span>)                           <span style=\"color: #00af00; text-decoration-color: #00af00\">650</span> \n",
              "\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m78,866\u001b[0m (308.07 KB)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">78,866</span> (308.07 KB)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m78,354\u001b[0m (306.07 KB)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">78,354</span> (306.07 KB)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m512\u001b[0m (2.00 KB)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">512</span> (2.00 KB)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Training MiniResNet...\n",
            "Epoch 1/30\n",
            "\u001b[1m45/45\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 19ms/step - accuracy: 0.3609 - loss: 2.0130 - val_accuracy: 0.2917 - val_loss: 2.1932\n",
            "Epoch 2/30\n",
            "\u001b[1m45/45\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 14ms/step - accuracy: 0.8886 - loss: 0.8093 - val_accuracy: 0.3778 - val_loss: 2.0845\n",
            "Epoch 3/30\n",
            "\u001b[1m45/45\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 14ms/step - accuracy: 0.9605 - loss: 0.2915 - val_accuracy: 0.3778 - val_loss: 1.9760\n",
            "Epoch 4/30\n",
            "\u001b[1m45/45\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 14ms/step - accuracy: 0.9808 - loss: 0.1298 - val_accuracy: 0.4000 - val_loss: 1.8097\n",
            "Epoch 5/30\n",
            "\u001b[1m45/45\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 14ms/step - accuracy: 0.9932 - loss: 0.0680 - val_accuracy: 0.4222 - val_loss: 1.8656\n",
            "Epoch 6/30\n",
            "\u001b[1m45/45\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 14ms/step - accuracy: 0.9954 - loss: 0.0501 - val_accuracy: 0.4639 - val_loss: 1.6341\n",
            "Epoch 7/30\n",
            "\u001b[1m45/45\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 14ms/step - accuracy: 0.9987 - loss: 0.0362 - val_accuracy: 0.6361 - val_loss: 0.8178\n",
            "Epoch 8/30\n",
            "\u001b[1m45/45\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 23ms/step - accuracy: 0.9991 - loss: 0.0225 - val_accuracy: 0.8194 - val_loss: 0.4100\n",
            "Epoch 9/30\n",
            "\u001b[1m45/45\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 14ms/step - accuracy: 0.9970 - loss: 0.0260 - val_accuracy: 0.8556 - val_loss: 0.3433\n",
            "Epoch 10/30\n",
            "\u001b[1m45/45\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 14ms/step - accuracy: 0.9986 - loss: 0.0219 - val_accuracy: 0.9833 - val_loss: 0.0732\n",
            "Epoch 11/30\n",
            "\u001b[1m45/45\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 14ms/step - accuracy: 0.9976 - loss: 0.0180 - val_accuracy: 0.9806 - val_loss: 0.0669\n",
            "Epoch 12/30\n",
            "\u001b[1m45/45\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 14ms/step - accuracy: 0.9988 - loss: 0.0113 - val_accuracy: 0.9889 - val_loss: 0.0404\n",
            "Epoch 13/30\n",
            "\u001b[1m45/45\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 14ms/step - accuracy: 0.9973 - loss: 0.0145 - val_accuracy: 0.9917 - val_loss: 0.0290\n",
            "Epoch 14/30\n",
            "\u001b[1m45/45\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 14ms/step - accuracy: 0.9991 - loss: 0.0093 - val_accuracy: 0.9889 - val_loss: 0.0307\n",
            "Epoch 15/30\n",
            "\u001b[1m45/45\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 14ms/step - accuracy: 1.0000 - loss: 0.0082 - val_accuracy: 0.9889 - val_loss: 0.0279\n",
            "Epoch 16/30\n",
            "\u001b[1m45/45\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 14ms/step - accuracy: 1.0000 - loss: 0.0066 - val_accuracy: 0.9944 - val_loss: 0.0225\n",
            "Epoch 17/30\n",
            "\u001b[1m45/45\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 15ms/step - accuracy: 1.0000 - loss: 0.0064 - val_accuracy: 0.9944 - val_loss: 0.0229\n",
            "Epoch 18/30\n",
            "\u001b[1m45/45\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 15ms/step - accuracy: 1.0000 - loss: 0.0052 - val_accuracy: 0.9917 - val_loss: 0.0252\n",
            "Epoch 19/30\n",
            "\u001b[1m45/45\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 14ms/step - accuracy: 1.0000 - loss: 0.0049 - val_accuracy: 0.9972 - val_loss: 0.0202\n",
            "Epoch 20/30\n",
            "\u001b[1m45/45\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 14ms/step - accuracy: 1.0000 - loss: 0.0040 - val_accuracy: 0.9944 - val_loss: 0.0320\n",
            "Epoch 21/30\n",
            "\u001b[1m45/45\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 14ms/step - accuracy: 1.0000 - loss: 0.0038 - val_accuracy: 0.9972 - val_loss: 0.0192\n",
            "Epoch 22/30\n",
            "\u001b[1m45/45\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 16ms/step - accuracy: 1.0000 - loss: 0.0034 - val_accuracy: 0.9972 - val_loss: 0.0191\n",
            "Epoch 23/30\n",
            "\u001b[1m45/45\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 23ms/step - accuracy: 1.0000 - loss: 0.0035 - val_accuracy: 0.9972 - val_loss: 0.0171\n",
            "Epoch 24/30\n",
            "\u001b[1m45/45\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 17ms/step - accuracy: 1.0000 - loss: 0.0023 - val_accuracy: 0.9944 - val_loss: 0.0171\n",
            "Epoch 25/30\n",
            "\u001b[1m45/45\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 15ms/step - accuracy: 1.0000 - loss: 0.0032 - val_accuracy: 0.9889 - val_loss: 0.0257\n",
            "Epoch 26/30\n",
            "\u001b[1m45/45\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 14ms/step - accuracy: 1.0000 - loss: 0.0025 - val_accuracy: 0.9917 - val_loss: 0.0248\n",
            "Epoch 27/30\n",
            "\u001b[1m45/45\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 14ms/step - accuracy: 1.0000 - loss: 0.0022 - val_accuracy: 0.9861 - val_loss: 0.0223\n",
            "Epoch 28/30\n",
            "\u001b[1m45/45\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 17ms/step - accuracy: 1.0000 - loss: 0.0028 - val_accuracy: 0.9917 - val_loss: 0.0209\n",
            "Epoch 29/30\n",
            "\u001b[1m45/45\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 14ms/step - accuracy: 1.0000 - loss: 0.0021 - val_accuracy: 0.9917 - val_loss: 0.0264\n",
            "Epoch 30/30\n",
            "\u001b[1m45/45\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 14ms/step - accuracy: 1.0000 - loss: 0.0022 - val_accuracy: 0.9944 - val_loss: 0.0185\n",
            "\n",
            "Test Accuracy: 99.44%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ============================================================================\n",
        "#                    VISUALIZE TRAINING RESULTS\n",
        "# ============================================================================\n",
        "\n",
        "# Plot training history\n",
        "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(14, 5))\n",
        "\n",
        "ax1.plot(history.history['loss'], 'b-', label='Train Loss')\n",
        "ax1.plot(history.history['val_loss'], 'r-', label='Val Loss')\n",
        "ax1.set_xlabel('Epoch')\n",
        "ax1.set_ylabel('Loss')\n",
        "ax1.set_title('MiniResNet Training Loss', fontweight='bold')\n",
        "ax1.legend()\n",
        "ax1.grid(True, alpha=0.3)\n",
        "\n",
        "ax2.plot(history.history['accuracy'], 'b-', label='Train Acc')\n",
        "ax2.plot(history.history['val_accuracy'], 'r-', label='Val Acc')\n",
        "ax2.set_xlabel('Epoch')\n",
        "ax2.set_ylabel('Accuracy')\n",
        "ax2.set_title('MiniResNet Training Accuracy', fontweight='bold')\n",
        "ax2.legend()\n",
        "ax2.grid(True, alpha=0.3)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "# Visualize predictions\n",
        "fig, axes = plt.subplots(2, 5, figsize=(15, 6))\n",
        "fig.suptitle('MiniResNet Predictions', fontsize=14, fontweight='bold')\n",
        "\n",
        "predictions = resnet_model.predict(X_test[:10], verbose=0)\n",
        "pred_classes = np.argmax(predictions, axis=1)\n",
        "\n",
        "for i, ax in enumerate(axes.flatten()):\n",
        "    img = X_test[i].reshape(8, 8)\n",
        "    true_label = y_test[i]\n",
        "    pred_label = pred_classes[i]\n",
        "    confidence = predictions[i][pred_label] * 100\n",
        "\n",
        "    ax.imshow(img, cmap='gray')\n",
        "    color = 'green' if true_label == pred_label else 'red'\n",
        "    ax.set_title(f'True: {true_label}, Pred: {pred_label}\\n({confidence:.1f}%)',\n",
        "                 color=color, fontsize=10)\n",
        "    ax.axis('off')\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 964
        },
        "id": "6QkXz-l8WcDA",
        "outputId": "86b35031-adb2-44c2-965d-58156a67f778"
      },
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 1400x500 with 2 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 1500x600 with 10 Axes>"
            ],
            "image/png": "iVBORw0KGgoAAAANSUhEUgAABZgAAAJNCAYAAAC8+RDWAAAAOnRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjEwLjAsIGh0dHBzOi8vbWF0cGxvdGxpYi5vcmcvlHJYcgAAAAlwSFlzAAAPYQAAD2EBqD+naQAAc0FJREFUeJzs3Xd4FFXDxuFn0wMpBAgtNGmhifQmShOQoiAW9FUQbIiCBVD0BRvSVLAiICKiIEUBKQJKFUSkvvQIKJ0gLZCEEELafH/sl4UlQcKQnCz6u6+LS3d3dp4zS3KYPJk967AsyxIAAAAAAAAAANfIK68HAAAAAAAAAAC4MVEwAwAAAAAAAABsoWAGAAAAAAAAANhCwQwAAAAAAAAAsIWCGQAAAAAAAABgCwUzAAAAAAAAAMAWCmYAAAAAAAAAgC0UzAAAAAAAAAAAWyiYAQAAAAAAAAC2UDADAADkga5duyoyMtL1Z/PmzZm2OX36tKpWrerapkWLFq7HPvnkE9f9R44csTWGI0eOuPbxySefuO5v0aKF29gu/VOzZk3ddddd+uCDDxQXF2cr1+4YIyMjNX369L/dZvbs2bazvv76a02aNOmq261bt+6Kr0+tWrV0zz33aPTo0Tp37pztsdgxe/bsLF+HjL/Prl272t73hg0b9Mknn7h9rV3p6wcAAAD/Lj55PQAAAABIixcvVq1atdzuW758udLS0rLcPiIiQvXr15ck+fv728r09/d37SMiIiLT4z4+Pqpdu7brdlpamg4dOqQ9e/Zoz549mj9/vr777jsVKlTIVr4dH374odq2bavQ0NAc3e/Ro0c1bNgwlShRQt27d8/284oVK6bSpUtLklJTU3Xo0CFFRUUpKipKixYt0vTp0xUcHJyjY71Wt9xyiyIiIlS5cmXb+xg9erTWrl2r+vXrq2TJkpKu/vUDAACAfwcKZgAAgDxUoEABxcbGavHixRowYIDbY0uXLpUkhYaGZrpauHPnzurcufN1ZYeHh2vy5MlXfDwoKCjT4+np6Ro1apQmTJig6OhoffHFF3r55ZevaxzX4syZM/r444/12muv5eh+f/rpJ1mWdc3Pa9eundvfW2pqqoYOHaqpU6fqzz//1OTJk/XMM8/k5FCv2QcffHBdzz99+rQ2bNiQ6f6rff0AAADg34ElMgAAAPJQxYoVFR4eriNHjmjnzp2u+8+dO6dff/1V3t7eqlevXqbnZbVExqVLFowZM0abN2/WI488olq1aqlevXoaOHCg27INdpY48PLy0pNPPum6vXXrVrfHt2/frj59+qhx48aqXr26mjZtqrfffluxsbFu2yUnJ+uzzz7Tvffeq0aNGqlGjRpq0aKFXn/9dR0+fDjL7Lp160qSpk2bpj179mRrvGvWrNHjjz+uBg0aqHr16rrjjjv0wQcf6Pz5826vwYgRIyRJ0dHR17WchI+Pj9vrk7H0yaXLasyZM0dDhgxRvXr19Prrr7u2PXDggF5++WXdfvvtql69uho3bqwBAwbor7/+ypQzefJktWnTRtWrV1eLFi00btw4paenZzmmKy2RERsbq3fffde1n9q1a+vRRx/Vb7/95tqma9euatSoketK+m7durm+5v7u6+evv/7S4MGDdccdd+jmm29WrVq11LlzZ33++ee6cOFCluPr3r27Tpw4oRdffFH169dXjRo11L17dx06dMhteztfOwAAAMg9FMwAAAB5yOFwqEmTJpKcy2RkWLlypZKTk1WjRg1bSyzs2rVLPXr0UGxsrAICAhQfH6+ZM2fq1Vdfve4xX7psx6VjW716tR566CEtXrxYycnJioyMVHx8vKZMmaKuXbu6Sl1J6tu3r95//31FRUWpcOHCqlq1qhISEjRjxgzdf//9io6OzpTbqVMnlSlTRmlpaRoyZMhVxzlz5kz16NFDq1evlsPhUKVKlXT8+HGNGzdOvXr1kmVZrmUeMo7Dz89P9evXv67lJC59fbJavuS7777TjBkzVLp0aYWFhUly/n117txZc+fOVVxcnCIjI5Wamqo5c+bogQce0IkTJ1zPnzhxooYMGaIDBw4oMDBQJUqU0IQJE/TVV19le4ynT5/WAw88oC+++EKHDx9WuXLllD9/fq1du1bdu3fX999/L0mqXLmybrrpJtfzKleurPr16//tsiy///67OnXqpG+++UZHjx5V2bJlFRYWpp07d2rkyJHq1q1bppJZkuLj49W9e3dt3rxZBQoU0IULF/Tbb7/p4YcfVnJysms7O187AAAAyD0UzAAAAHns9ttvl+RcpiFDxvIYzZo1s7XPn376SUOHDtUPP/yg5cuXuwrTxYsX68yZM7bHmp6errFjx7puZ4w9LS1Nr7/+ulJSUhQREaElS5Zo1qxZWrRokQoUKKA9e/a4llM4c+aMlixZIknq06eP5s+fr+nTp2vZsmWqWbOmypYtm+nKaEny9vbWK6+8Isl5RfCiRYuuOM64uDgNGzZMklSjRg2tWLFCs2fP1nfffSdfX1/99ttvWrRokWuZhypVqki6uOzDwIEDbb0+KSkp+vTTT123M355cKmtW7fqu+++06xZs/Tiiy9Kkt566y2dO3dOQUFB+uGHHzRr1iwtXbpUN910k06cOOHaZ3JyssaMGSNJKliwoH744QdNmTJFCxcuvKYPXfzwww918OBBSdLHH3+sefPmafny5WrcuLEkafDgwUpMTNTAgQP11FNPuZ733//+V5MnT1Z4eHiW+7UsSwMGDFBsbKz8/f01ffp0zZ8/X8uXL9fTTz8tSdqyZYsmTJiQ6bk7d+5UvXr1tGLFCi1evFj333+/JOnEiRP6+eefJdn/2gEAAEDuoWAGAADIY7fffrv8/Py0f/9+/fHHH0pOTtbKlSslSa1atbK1z8qVK6t9+/aSpMDAQNf/W5aV7WUEEhIS1LVrV9efhx9+WM2aNXMVxc2bN9cDDzwgSdqxY4frytH27du7rswtVqyYqyTPKND9/Pzk4+P8KJBFixZp4cKFOn78uIKDgzVjxgxNnz5d7dq1y3JMLVq0cJW27777rpKSkrLc7tdff3UtB3LvvfcqMDDQ9brUrFlTkvTjjz9m63X4OwsXLnS9Pg899JBuv/1219W/9evXz3Kd7KZNm7pdIX3q1Cn973//cz1WqlQpSVJISIjrdch47fbs2aOzZ89Kcq7/XLRoUUlSkSJFdM8992RrzOnp6a5yvmzZsrrjjjskSb6+vho8eLDGjRun999/3+2q4ezavXu3du/eLUlq27atatSo4XqsV69eCggIkJT1a+9wONS3b185HA5JchXMklzLZFzP1w4AAAByBx/yBwAAkMeCgoJ06623asWKFVqyZInrLf8VKlRQ+fLlbe2zQoUKbrcLFSrk+v9Ll6r4O6mpqVq/fn2m+729vTVy5Ejdeeed8vJyXq9w6bIE48eP1/jx4zM9L2Pd5Pz58+ull17SiBEjtGfPHtdVvBEREWrYsKEefvhhVatW7Yrj+u9//6uOHTvq6NGjrrV4L5exLrUkvfHGG3rjjTcybZNRhF6PY8eO6dixY67bgYGBqlq1qjp06KCuXbvKz88v03PKlCnjdvvS127BggVasGBBpuecOXNGJ06ccFuPOaOIznDpUhZ/58yZM4qPj5cklS5d2u2xUqVKZdrvtdi3b5/r/8uVK+f2WEBAgIoVK6YDBw64rp6+VOHChRUaGuq6XbBgQdf/Z3zNXu/XDgAAAHIeBTMAAIAHaNWqlVasWKE1a9a41ttt3bq17f35+vq63c64KvRaFChQQOvWrXPdHj9+vEaNGqW0tDTt27fPVS5frkyZMq4ray+XnJwsPz8/de/eXbfddpvmzZun9evXKyoqStHR0Zo1a5bmzp2rDz/88IpXb5cvX14PP/ywJk2apC+++MK1rMOVVKpUSQUKFMh0f0hIyN8+Lzsee+wxDRgw4Jqek3E1dVaKFSuWqfTNkJKSIsuyXLcvv8L40rWf/86l+7jSBwPmhKz2nXFfVl87l5fxV/qavZ6vHQAAAOQ8CmYAAAAP0LJlS/n4+GjLli2uq2/btGmTx6Ny16NHD82bN09//PGHxo0bp1atWikyMlKSVLJkSdd27dq10wsvvHDV/ZUvX951BWpqaqo2bNigl19+WSdOnNDYsWP/tiTs3bu35s2bp9OnT7utCZ3h0qtwu3Xr5rbcgqe59LWrX7++3nvvvStue+rUKdf/ZywbkSG7V2QXLFhQ+fPn17lz57Lcx6XLs2T3qugMl161/Oeff7o9lpCQ4LoC+/Krm6/V9XztAAAAIGexBjMAAIAHKFCggOrXr6+UlBT99ddfKlOmjNs6vZ7A19dXb775phwOh1JSUvTqq68qNTVVklStWjWVKFFCkjRv3jydPHlSkvOq24EDB+q5557TF198IUlas2aNOnfurCZNmrgKTh8fH9WvX18RERHZGktwcLD69u0rybne8uUaN26sfPnySZJmzJihhIQESc6S87nnntPzzz+v2bNnu7b39vaWJJ0+fVqJiYnX9sJcp0KFCql27dqSpOXLl2v//v2SnFcav/fee+rdu7erdK5cubJrfeuffvrJtZ72/v37NWfOnGzleXl5ua6OP3TokBYuXCjJWdS+9957GjVqlD7++GPX65fx2kjuS49kJTIy0vWBiYsXL9b27dtdx/LJJ58oJSVFktSxY8dsjfVyOfG1AwAAgJxFwQwAAOAhLl0S43qWx8hNdevW1X333SdJ2rlzp6s09vb21ptvvikfHx9FR0erdevWuv/++9W8eXPNnDlTK1eudH24Xo0aNRQXF6eTJ0+qffv2uvfee/XQQw+padOm2rx5syTnVcdXc++9915xvd3Q0FC98sorkqTt27e7PpCwZcuW+umnn7Ru3TrVqlXLtX3GWtfnz59X+/bt9fTTT9t7gWx67bXXlC9fPiUkJOjuu+9W586d1bJlS02YMEHLli3TzTffLEny9/d3vTbx8fG666671LlzZ3Xq1Ml1NXl29OvXz1XI9uvXT3fffbdatGihX375RZLUt29f1zInl15tPHjwYHXp0kXbtm3Lcr8Oh0MjRoxQgQIFlJycrIceekidOnVS06ZNNWnSJElSs2bN9Mgjj1zbC/T/cuprBwAAADmHghkAAMBDtGrVyrU2racWzJLUv39/1wewjR492rUUQtOmTTV16lS1aNFC/v7+2rlzp9LT09WmTRtNmzZNderUkeT8UMOZM2fqscceU6lSpXTgwAHt2LFDfn5+at68ub788kt16tTpquPw8vLSwIEDr/h4ly5d9Pnnn6tRo0aSnIW4v7+/OnfurG+//dZt+YeePXuqUaNG8vf3V2xsrM1Xxr6qVatq5syZ6tChg0JDQ7V7924lJCSoadOm+vLLL3XnnXe6tn366af1/PPPq1ixYkpNTVViYqL69eunJ554Itt54eHhmjlzprp166aIiAjt27dP586dU+PGjTV58mQ99thjrm1vvvlmPfPMMwoLC1NaWppiYmKy/PDCDJUrV9b333+vBx98UEWKFNGff/6pc+fOqVatWho8eLDGjBnjdlX0tciprx0AAADkHId16ad8AAAAAAAAAACQTVzBDAAAAAAAAACwhYIZAAAAAAAAAGALBTMAAAAAAAAAwBYKZgAAAAAAAACALRTMAAAAAAAAAABbKJgBAAAAAAAAALZQMAMAAAAAAAAAbKFgBgAAAAAAAADYQsEMAAAAAAAAALCFghkAAAAAAAAAYAsFMwAAAAAAAADAFgpmAAAAAAAAAIAtFMwAAAAAAAAAAFsomAEAAAAAAAAAtlAwAwAAAAAAAABsoWAGAAAAAAAAANhCwQwAAAAAAAAAsIWCGQAAAAAAAABgCwUzAAAAAAAAAMAWCmYAAAAAAAAAgC0UzAAAAAAAAAAAWyiYAQAAAAAAAAC2UDDDiHEbx6nsh2XzehgAkMkrS19Rs0nN8noYAJAJ508APBlzFABP9eOfP8rxliOvh/Gv4pPXA7jRtJ7cWqsOrpIkpaanKt1Kl5+3n+vx3b13q0yBMkbHlJKWopeXvKyvt32tlLQUtS7fWuPvGq+CgQWv+tzuc7pr8rbJ8vXylSR5e3nrpgI3qU/9PupZt2duD/2K1hxeoz6L+ijqZJRKhpTUW83e0n9u/k+ejQe4EXji/LRk7xINWjFIUSejFJ4vXG81e0tdb+marec2m9RMqw+tlo+X858qX29fRRaK1H9v+686V+mcm8O+ouMJx9VvcT8t3bdUSalJ6lylsz5t96kCfQPzZDzAjcIT56cV+1fo1WWvaufJnQrxD1H7iu01qvUoBfsHX/W5nnj+dCH1gl5a8pK+i/pOCckJiiwUqbebv622FdvmyXiAG4mnzVGTt07Wk/OfdLsv3UpXREiE9j+//6rP98Q5KnJ0pA7GHnS7LzktWV92/FKP1nw0T8YE3Ag8bX6SpG3Ht6nvT3218ehGBfkF6b6q9+ndVu+6jetK3vz5TQ1eOdi1rcPhUKmQUupes7sG3DpA3l7euT38LI3bOE4frP1A0fHRqlCwgt5q9pY6Vu6YJ2O5UVEwX6PFXRe7/v/Nn9/Uj3/+qLVPrM3DEUn/XfZfbfxro7Y9vU3+Pv7qvbC3Pt/0uQY0GZCt599f9X5Nv2+6JOeEtWL/CnX+trNCA0L1YPUHc3PoWfrr7F/qMLWDPrrzI91f7X6t2L9CLy15SXdWuDNbpTnwb+Vp89MfMX/orml36f027+vxWo9rw9EN6ji9oyoVqqQGJRtkax/9G/fXiDtGSHKWJ7N/n60HZz6on7v/rMalGufm8LP0n9n/kY+Xj7Y+vVXeXt7q+n1X9V/cX5+2/9T4WIAbiafNT3+d/Uvtp7bXp+0+VddbuupI/BG1+6adXl/xuj6484Ns7cPTzp8GLB2g9dHrteHJDSoWVEyfrPtEnb/trP3P71exoGLGxwPcSDxtjup6S9dMv5B/av5TCgsIy/Y+PG2O2t17t9vtfWf2qdEXjXRnhTuNjwW4kXja/JSQnKA2U9rosZqPacF/Fmh/7H61/aatCucrrEG3D8rWPupH1HcdQ7qVro1HN6rzjM7ycnjplSav5ObwszQrapZeWfqKFvxngepH1NfXW7/WAzMf0O/P/q5yYeWMj+dGxRIZucDxlkMf/PaBio8qrhGrR2jSlkkqNtL9xL7hhIZ68+c3XbdHrx+tKp9WUb6h+VRtTDXN3TXX9diQVUPUdFLTLLPOp5zXmI1j9NGdHykiJEKF8xXW9PumZ7tcvpyPl49alW+lB6s9qNm/z5bknMQ6TO2gLjO7KGR4iCu398LeKv1BaeUfll/Nv2quqJNRrv2sO7JOt4y7RfmH5Verya104twJt5yAIQFasndJlmMYv2m8mpRuoq63dFWAT4DaVmyrHc/soFwGcoDJ+Wnx3sUqGVJSz9R7Rv4+/mpSuoker/W4Jm6eaGvs/j7+eujmh9S0bFPN2TVHkvMKnSfmPaFmk5qp+pjqkqTT50/rkdmPqPio4goeHqyO0zsqOj7atZ/5u+crcnSkgoYFqcvMLkpMSXQ9djD2oAKGBGhPzJ5M+QnJCVqxf4Veu/01FQ0qqsL5CmtU61H6etvXSk5LtnVMAC4yOT+lpqdq/F3j1aNWD/l4+ahsgbK6s8Kd2nFyh62xe8L5U4ubWuiLu79QyZCS8vHy0eO1H1dSapL2nt5r65gAuDM5R11uQ/QGLfhjQbbLm8t5whx1ued/fF79G/VX0aCito4JwEUm56fjCcfVtkJbvdX8Lfn7+Kty4cq6t8q9rqusr5WXw0v1I+qrV91ervlp0pZJqj6muvr91E/5h+XX0bNHlW6l640Vb6j8x+WVb2g+1fu8nn499KtrP3/E/KFbJ96qoGFBajChgf6I+cMtJ3J0pCb8b0KWYzifel7DWw7XraVvla+3rx6v/biC/YK19kjeXkx6o6FgziVzds/Rlp5bNODWqxe9s3+frbdWvqUp90xR/Kvxerv523pg5gM6FHdIkjTo9kFa2X1lls/931//U0painac2KFyH5VTkfeK6Ml5T+pc8rnrGn+aleb21oS1R9aqWZlmOjPgjCTnVTKbj23W2ifW6tRLp1SvRD11ntFZlmUpLT1N9313n9qUb6OYl2M0pPkQjd803m3/SYOS1Kp8qyyzVx9erXJh5dRpeieFjghVzXE1s32iAuDqTM1PkvMtT5cKCwjTluNbrmv8aelp8nZcnJ/m7p6r/o37a3uv7ZKcpXNiSqKinolSdN9oBfkFqcfcHpKk2KRYdZnZRb3r9dbpAafV/Zbu+nrr1659lSlQRkmDklSpUKUrH5MuHlNYQJgSkhMocIAcYmp+KhVaSo/UeESSZFmWNh3dpNm/z1aXal2ua/x5ef50d+TdqlakmiQp/kK8hv8yXBULVlTt4rWv65gAXGTyHOpS/Zf018DbBmZrCZ+/k5dz1KVW7F+hLce26PmGz1/X8QC4yNT8VL5geU3sONG1jKEkHY4/rIiQiOsa/+Xz09GzRxXoG6jYAbEqEVxCH679UNN2TNOPD/+o2Fdi1a1GN9017S5X9/XonEdVJrSMjvc/rq86faXPNn3mtv/dvXfridpPZJn9SI1H1KteL9ft2KRYnU0+q4jg6zumfxsK5lzyQNUHVDSoaKZyJStfbP5Cj9d6XHVK1JGPl486V+msJqWbaNr2aVd97pH4I5KcC5hvfGqjVnZfqZ8P/qyBywfaGndKWoqW7F2ib3d+6/ZDlreXt56u+7S8vbyVbqVr0pZJeu3211QiuIQCfQM1pMUQHYw7qPXR67Xx6EYdPXtUA28bqACfADUo2UD3VL4n22M4En9Ek7dNVu/6vXW071HdX/V+dZrRSUfPHrV1TADcmZqf2lRoo4OxBzV2w1hdSL2grce2avK2yTp9/rStcSelJmnq9qlafWi17q16r+v+sgXKqkOlDnI4HDpx7oTm75mvYS2HKSwwTCH+IRrRcoSW7FuiYwnH9NOfPynIL0jP1n9Wft5+aluxrW4rc1u28oP8gtS0bFO9tfItnTh3QmfOn9EbP78hHy8f28cEwJ2p+SnDqoOr5DfET42+aKQeNXtc8QePq/GE86cMrSe3VuiIUC38c6HmPTSPNeKBHGR6jpKkXw/9qj0xe/RYrcfsDtuj5ihJGvrLUPVr1C9b67UCyJ68mJ8kad7ueZq/e776N+pvZ9hKS0/TuiPr9Nmmz9zmp7gLcXr51pfl6+3rGnPfRn1VsVBF+Xn7qU+DPgoLDNMPe37QsYRj+u3Ib3q1yavK75dflQtXVo+aPWyNx7IsPTn/STWIaKCmZbP3LhM4sQZzLrmWRdb3nt6rxXsX68O1H7ruS7fSVbVw1as+15KllPQUDWkxRAUDC6pgYEH1b9Rfb618Sx/e+eFVny9J30V9pzlD5khyvn2qYqGKGtN+jDpV7uTaplRIKddEdeLcCZ1NPquO0zu6XcmXZqXpcPxhOeRQWECYQgNCXY/93dWAmY7JstS+YnvdUe4OSdKrt72qMRvH6Ic9P+ipOk9lez8AsmZqfqpQsIK+vf9bvb7idQ1YOkCNSjVS95rd9eWWL7OdP3LNSFe2n7efqoZX1dwH56puiboXjyf04vHsO7NPklRzXE23/Xg7vHU47rCOxB9R6dDS8nJc/P1qpYKVtOmvTdkaz9edvlbvRb0VOTpShfMV1uBmg/XN9m/cfoMPwD5T81OG28vcrguDLmj78e165PtHdCHtgoa1HJat53ra+VOGxV0XK/5CvMZuGKvbv7xdW57eohLBJa55PwAyMz1HSdIHaz/QU7WfUoBPwDU9z1PnqB0ndui3I79p7oNzr74xgGzLi/lp9u+z9eicRzX5nsmud1Flx/ro9QoY4pzTvBxeKlugrPo27KvnGjzn2iYswHmx0KVjfm7Rc3rhxxdc92XMTxnLId4UdpPrMTvzU0pairrP7a6dJ3ZqxaMrrvn5/3b8RJxLrlY2pFlprv8P9A3UiJYj1K9xv2vOyfjQlgIBBVz3lS1QVifOnZBlWdn67dWlHwBxJZceT6CP80qYNY+tUZ0SdTJtO3X7VKWmp7rdl26lX3UcGYoFFXM7Hi+Hl0qHltaxhGPZ3geAKzM1P0lSp8qd3H6QGbVm1DW91ejSD/m7kqzmp+i+0SqUr1CmbZfsW3Jd81Op0FJuPxDFJMYoMSXxut8SBsDJ5PyUwcvhpVuK3aL/NvmvnvrhKQ1tMfSGPH+6VIh/iAY0GaCJWyZq6vap6t/Y3lVFANyZnqMSUxK18I+FerXJq9f8XE+do77b+Z1a3NRC+f3yX/NzAVyZ6flp/KbxGrB0gGY9MEuty7e+pude+iF/V3L58QT6BmrCXRPc3smaYc3hNZLkNkdd6/x0PuW8Ok7vqMSURP3S45csf5bE32OJDAMCfALcPkQqLT1NB2IPuG6XDyuvbSe2uT3nUNwhWZZ11X1XKVxFDjm05dgW130HYg+oVGipbP1wZEdoQKgKBRbStuPuY844phLBJRR/IV5xSXGuxy79cIirqRpe1e14LMvSobhDblcpAsgZuTk/nTl/Rl9u/tJt28X7FqtxqcbXP/ArKFugrLwcXm7zU0paimuJnRLBJRR9NtptTFGnsj8/LdizQL+f/N11e/HexSodWlolQ0rmwOgBXCo356evt36tZpOaud3n5fCSj5fPDXv+VOuzWpq3e57bfV4OL/l6+dofNIArys05KsPivYuVzzefkbXUc3uOyjB391y1LndtZRSAa5Pb89PMqJkauHygVjy64prLZbvKh5X/2/lJkg7HHXY9di3zk2VZenDWg/L19tXSbkspl22iYDagYsGKOpt8Vov3LlZyWrKGrx7u9o3bs05PzdgxQwv2LFBqeqpW7F+h6mOqa130uqvuu2hQUXWq3EmvLntVxxKOaf+Z/Xp/7fuu9Wai46NVeXRl7T+zP0ePqWednhryyxDtOrVLKWkp+uC3D1Tv83pKTElUg4gGCgsM07u/vqsLqRe0+tBq/fDHD9ne95O1n9RvR37TV1u+UlJqkkauGanzKefdroIEkDNyc37y8fLR8z8+rzEbxigtPU1fb/1avx3+TT3r9JTkfGtU5dGVlZyWnGPHExoQqgerP6gBSwfoSPwRnU85r1eXvapWk1vJsizdUe4OxSXF6bNNnyk5LVlzd83VuiNXP5YM30V9p2cXPqv4C/Had2afBq0YpH6Nru/qSQBZy8356bbSt2l99Hp9vO5jXUi9oIOxB/Xemvd0V6W7JN2Y508NIxrqtRWvae/pvUpJS9H4TeO178w+tanQJkePAYBTbs5RGTb/tVllC5TN9IuvG3GOkqTktGTtPLnT7W3sAHJebs5PcUlx6rWgl6bcM0U1i9XMcpvKoytr9aHVOXU4rjF/uuFTrT2yVmnpafp257eqNqaaDsUdUtkCZVWlcBWN/G2kElMStePEDk3eNjnb+566fap2ntip7+7/7pqXI8JFFMwG1ClRRy82fFFdZnZRxPsR8vXydbuCr1X5VhrZeqR6L+qt4OHBenbhsxrbfqwalmwoSRqyaoiaTrry4uITO05UubByqvRJJdUeX1t3VbrL9TaqlPQU7Y7ZrZT0lBw9pteavqY7y9+pJhObqNC7hfT9ru+16OFFyuebT4G+gZrTZY7m7p6rsHfC9ObPb2YqYAKGBGjJ3iVZ7rtW8Vqafu90Df1lqAqMKKCpO6bqp0d+clvvC0DOyM35Kdg/WN/e/61GbxitoOFB+mDtB1rwnwWu5SQSUxK1O2Z3jh/TJ20/UYWCFVRtTDWVeL+Eok5Gae6Dc+VwOFQypKSm3TtNI9eMVNg7YZqyfYqeqfeM67kHYw8qYEiA9sTsyXLfo1qPUj7ffIp4P0KNv2isbjW6qU/9Pjl+DAByd366Kewm/fjIj/pq61cKHRGqRl80Up3idfRJ208k3ZjnT6PajFLzss3VYEIDhb0TpvGbxuv7Lt+rcuHKOXoMAJxy+2c8STqWcMy1JOKlbsQ5SnIuLZaanprlMQHIObk5P83bPU+nEk+p4/SOChgS4PYnw+6Y3W5XUOeEx2s/rmfqPaPOMzorZESI3vn1HX3f5XuVDi0tSZr5wEztOrVL4e+Fq8fcHnqp8Utuz48cHakJ/5uQ5b4nbpmoA7EHVPCdgm7H8+S8J3P0GP7pHNa1vEcHN6Ru33fTyNYjVSR/kbweCgC4aftNWy16eFFeDwMAMuH8CYAnY44C4KleX/G6OlTqoPoR9fN6KDCIK5j/4ZJSk3Qg9gAnHgA8zrGEY/Lz9svrYQBAJpw/AfBkzFEAPNnKgyt1S9Fb8noYMIwrmAEAAAAAAAAAtnAFMwAAAAAAAADAFgpmAAAAAAAAAIAtFMweIjYpVuU/Lq+l+5bm9VBsuZB6QdXHVNe07dPyeigADPD0OcuyLLWa3ErDfxme10MBkMs8fT66Gs6hgH8u5icAnow5CjmJgtlD9FrQS3eWv1N3lLtDlmVp5JqR8nvbT+M2jnPbLt1K18BlA1Xuo3IKeydMd065U/vO7HM9fvr8aXWZ2UVFRxZV8VHF9cS8J3Q+5fwVc2fsmKEaY2soeHiw6oyvo8V7F7semxk1U8VHFVfxUcX1/e/fuz1vffR6VR5dWUmpSZIkfx9/fdXpK/Va0EuH4w7nxEsCwINdOmdN2z5NNcbWUP5h+VVtTDW3eeTshbPqvbC3Sr5fUkHDgtR5RmedSjx1xf1+t/M7177KfFhGA5YMUGp6qiRp1cFVKvdRORV6t5DGbBjj9ryDsQdV+oPSOnnupCTJ4XDoy45f6p1f39Gmo5ty4RUA4Ck4hwLgqZifAHgy5ijkKAt5btuxbZbf237W4bjDlmVZVrtv2lltp7S1irxXxBq7Yazbth+v/dgq+2FZK+pElBWfFG/1XtDbqjG2hpWenm5ZlmV1ntHZav9Ne+vkuZNWdHy01fiLxlafhX2yzN3812bL/21/a8GeBdb5lPPWlK1TrHxD81mH4w5baelpVtH3ilqb/9psbflri1ViVAlXRkpailVzXE1r2b5lmfZ519S7rpgH4J/h0jlr5YGVls9gH2t21GzrQuoFa+6uuVbI8BDrYOxBy7Is67E5j1k1x9W09p7ea8UnxVs95vSw2n3TLsv9bozeaAUOCbQW7llopaWnWduPb7eKvFfE+vC3Dy3Lsqy64+tac36fYx2NP2oVeqeQdeb8GddzO0ztYE3838RM++yzsI9119S7cv5FAOAROIcC4KmYnwB4MuYo5DSuYPYAYzeOVZvybVQypKQkqVHJRlrwnwUK9AnMtO1nmz7Tiw1fVJXwKgr2D9awlsMUdTJK66LX6XjCcc3ZNUfDWg5T4XyFVSK4hF67/TV9ueVLpaSlZNrXhP9NULuK7dSuYjsF+ATo4RoP6+YiN2vKtik6nnBcklSzWE3dUuwWpaSl6Pg5530frf1ItxS9RS1uapFpnz3r9NTEzROVnJacky8RAA9y6Zw1f/d8NS3TVPdUuUd+3n66O/JutSnfRt9s+0aSNG/PPPVr1E/lwsop2D9YH935kX768ycdPXs0037z+ebT1Hunqm3FtvJyeKl6keq6tdSt2nFihyRp2/FtalOhjYoHF1e5sHLadWqXJGlW1CwlJCeoR60emfbZs05P/bDnB0XHR+fiKwIgr3AOBcBTMT8B8GTMUchpFMweYNn+ZW7fJINuHySHw5Fpu/Mp5xV1Mkq1i9d23RfsH6yKBStqQ/QGbTm2Rd4Ob91c5GbX47WL11ZCcoKriLnUpr82ue0rY/sNRzfI4XAo3Up33W/JkkMOHYo7pE/Wf6L7qt6n2768TY2+aKQFexa4trutzG1KSk3S+uj19l4MAB7v8jnr8vkqLCBMW45vufi4Lj6ezzef/Lz9tPXY1kz7rRJeRZ0qd5IkpaWnadm+Zfrl0C+6t+q9rv1kzEsZc1L8hXi9vPRl9WvUT3d8fYcaTGigiZsnuvZZrUg1Fc5XWCsOrLju4wbgeTiHAuCpmJ8AeDLmKOQ0CuY8lpKWoj0xe9y+Ga/kTNIZWbIUFhDmdn/BwII6lXhKMedjFBoQ6jYpFAwsKElZrnkakxhzxX0VzV9Uft5+WndkndYcXqMgvyAVDSqq3gt7a3DzwXpl6Ssa3nK4vr3vWz05/0nXb6ZC/ENUKrSU64pDAP8sl89ZHSp10Ir9KzR311wlpyVr1cFVmr9nvk6fP+16/L017+lA7AGdSz6nN35+Q5Ys1+NZmbx1svyH+KvTjE4a2mKo7qxwpyTniccPe37Q/jP7dSD2gKqGV9Wg5YP06C2PatzGcepes7uWdF2i11e8rhPnTrj2V61INeYk4B+IcygAnor5CYAnY45CbqBgzmMZJUvGN2B2WLKu/Jh15ceuZV8Oh0Nj2o/Rvd/eqy4zu2hMuzGa/ftsJaYkqmNkRx09e1RNSjdRqdBSKhZUzO03U4XzFXZ90BaAf5bL56ymZZvq03af6qUlLyn8vXCNXj9a3W7pJh8vH0nS+63fV42iNVTv83qq8mkVhecLV7mwcq7Hs9L1lq5KGpSkRQ8v0tur3tZnGz9z7qvN+xq4fKAaTGigd+94V7tjdmvFgRV6pckrWnN4je6OvFsh/iGqH1Ff646sc+2POQn4Z+IcCoCnYn4C4MmYo5AbrvwTPozK6q0IlysYWFBeDi/FJMa43R9zPkZF8hdReL5wxV2IU1p6mry9vJ2P/f+2RfIXybS/8PzhmfeVGOPa9u7Iu3V35N2SpLMXzqr2+Npa9PAixV+IV5BfkOs5+f3yK+5C3MVjkeNvJx8AN75L56yedXuqZ92ertt9FvZRRHCEJCksMExf3/O16zHLsvTaitcUERLxt/v38fJRk9JN9EzdZ/TJ+k/Us25PNSzZUH/0+UOScwmNBhMaaGz7sfLz9lPchTjXvMScBPy7cA4FwFMxPwHwZMxRyElcwZzHMn5jdPk3WFYCfAJUvUh1bfprk+u+2KRY/Xn6TzUo2UC1iteSZVnaevzi2qYbjm5QgYACiiwcmWl/dYvXddtXxvYNIhpk2nbQ8kHqUbOHKhSsoBD/EMUmxboei0mMUbBfsOv2ycSTCs8XftXjAXDjuXzOOhJ/RNO2T3PbZsm+JWpcqrEkadXBVW5rYa09slap6amqVaxWpn0P/2W4Hpn9iNt9Xg4v+Xr7Ztr243Ufq3bx2mpSuokk59uizpw/4xobcxLwz8c5FABPxfwEwJMxRyE3UDDnMV9vX1UqVCnba8X0qttLH637SLtO7dLZC2c1YMkA1SpWS3VL1FXhfIV1X9X7NGj5IJ1KPKUj8Uc0eOVgPVHrCdfb0Vt+3VIzdsyQJD1Z50kt2bdEC/YsUFJqkiZunqg9MXv0SA33gmfT0U36+eDPeqnxS5Kk0IBQRYRE6Mc/f9T249t1/NxxVQmvIsn5G6bDcYd1c9Grr+UD4MZz+ZyVlJqkbnO6af7u+UpNT9XQVUN1LuWculTrIklavn+5esztoeMJx3Xi3Am98NMLerru08rvl1+S1O37bnr/t/clOZfb+Hbnt5oZNVOp6anaeWKnxm4cq7sq3eU2hsNxh/Xphk/1zh3vuO5rWLKhvov6TkfPHtX66PWqH1Hf9VjUySjmJOAfiHMoAJ6K+QmAJ2OOQq6wkOd6/dDLunva3ZZlWdbKAyst/7f9Lf+3/S29KctnsI/l/7a/1errVpZlWVZ6err1+vLXrSLvFbEChwRa7b5pZx2OO+zaV+z5WOvBmQ9aQcOCrLARYdazC561LqRecD1e5oMy1tgNY123Z0XNsip+XNHye9vPqjmuprXywEq3saWmpVp1x9e1fj30q9v9P+//2Sr9QWmr+Mji1pzf57juX7BngZV/aH63TAD/LJfOWZZlWV9t+coq80EZK3BIoNVkYhNrx/EdrsfOp5y3Hpn9iBUyPMQq+E5Bq/eC3m7zQ9Mvm1oDlgxw3Z4VNcuqPLqy5f+2v1X6g9LWgCUDrKSUJLf8jtM6WtO2T3O7b8fxHVbVT6tahd4p5DbH7Tyx03K86bCOxB3JseMH4Dk4hwLgqZifAHgy5ijkNIdlXeNq3Mhx245vU73P62nfc/uuui6pp+s0vZNKh5bWx20/zuuhAMglN9Kc9cKPL2jfmX2a99C8vB4KgFxwI81HV8M5FPDPwvwEwJMxRyGnsUSGB6hRtIY6V+msEatH5PVQrsvmvzZr5cGVrrcwAPhnulHmrOj4aH219Su90fSNvB4KgFxyo8xHV8M5FPDPw/wEwJMxRyGnUTB7iLHtx2rhnwu1bN+yvB6KLRdSL6jbnG4a026MSoWWyuvhAMhlnj5nWZalHnN76OXGL6tOiTp5PRwAucjT56Or4RwK+OdifgLgyZijkJNYIgMAAAAAAAAAYAtXMAMAAAAAAAAAbKFgBgAAAAAAAADYQsEMAAAAAAAAALCFghkAAAAAAAAAYItPdjd85plncnMcmcTExBjNk6Rt27YZzUtPTzeaJ0mtW7c2mvfzzz8bzZOkihUrGs0rUKCA0TxJmjhxovFMT9a/f3+jeQcOHDCaJ0mffvqp0bwTJ04YzZOk9u3bG81LSEgwmidJt956q9G8iIgIo3mSNG7cOOOZnm706NFG844dO2Y0T5LuuOMOo3lBQUFG8yRpw4YNRvOOHj1qNE+SLly4YDQvPDzcaJ4kvfTSS8YzPVmTJk2M5p07d85onmT+3/tixYoZzcsL5cuXN55ZuXJlo3mxsbFG8yRpxIgRxjM9Xffu3Y3m9ezZ02ieJJ09e9Zo3p9//mk0T5Lmz59vNG///v1G8ySpXLlyRvNCQ0ON5knStGnT/vZxrmAGAAAAAAAAANhCwQwAAAAAAAAAsIWCGQAAAAAAAABgCwUzAAAAAAAAAMAWCmYAAAAAAAAAgC0UzAAAAAAAAAAAWyiYAQAAAAAAAAC2UDADAAAAAAAAAGyhYAYAAAAAAAAA2ELBDAAAAAAAAACwhYIZAAAAAAAAAGALBTMAAAAAAAAAwBYKZgAAAAAAAACALRTMAAAAAAAAAABbKJgBAAAAAAAAALZQMAMAAAAAAAAAbKFgBgAAAAAAAADYQsEMAAAAAAAAALCFghkAAAAAAAAAYAsFMwAAAAAAAADAFgpmAAAAAAAAAIAtFMwAAAAAAAAAAFsomAEAAAAAAAAAtlAwAwAAAAAAAABsoWAGAAAAAAAAANhCwQwAAAAAAAAAsMUnuxumpaXl5jgyOXPmjNE8SUpJSTGaN2TIEKN5kjR16lSjeTExMUbzJKlMmTLGM5G3TH/vfvbZZ0bzJOnkyZNG8959912jeZI0atQoo3lRUVFG8yRp5cqVxjOR91JTU43mdevWzWieJJ06dcpoXr169YzmSdKXX35pNC8wMNBoniTFx8cbz0TeMv0zXmJiotE8SQoJCTGaV6xYMaN5knT27FmjeXnx99igQQOjec2bNzeah6yZnqP27t1rNE8y/zPJhg0bjOZJUteuXY3m9e/f32ieJJUsWdJ4pqfhCmYAAAAAAAAAgC0UzAAAAAAAAAAAWyiYAQAAAAAAAAC2UDADAAAAAAAAAGyhYAYAAAAAAAAA2ELBDAAAAAAAAACwhYIZAAAAAAAAAGALBTMAAAAAAAAAwBYKZgAAAAAAAACALRTMAAAAAAAAAABbKJgBAAAAAAAAALZQMAMAAAAAAAAAbKFgBgAAAAAAAADYQsEMAAAAAAAAALCFghkAAAAAAAAAYAsFMwAAAAAAAADAFgpmAAAAAAAAAIAtFMwAAAAAAAAAAFsomAEAAAAAAAAAtlAwAwAAAAAAAABsoWAGAAAAAAAAANhCwQwAAAAAAAAAsIWCGQAAAAAAAABgCwUzAAAAAAAAAMAWCmYAAAAAAAAAgC0UzAAAAAAAAAAAWyiYAQAAAAAAAAC2UDADAAAAAAAAAGzxye6GDocjN8eRyYULF4zmSVLXrl2N5p06dcponiT9/vvvRvN8fX2N5klSenq60TwvL35Pk9dMz0+mv8Ykydvb22jelClTjOZJUr169Yzm9e3b12ieJK1YscJonunvDWTN9N/D0KFDjeZJ0oIFC4zmtWnTxmheXmT269fPaJ4kFSxY0Ggec9S/T1pamvHM8PBwo3kxMTFG8yRpx44dRvMiIyON5knS1q1bjea9+uqrRvMkae3atcYzPZ3pfydGjx5tNE+S1q1bZzSvV69eRvMkKTU11Wjev6GH8sRzKJoxAAAAAAAAAIAtFMwAAAAAAAAAAFsomAEAAAAAAAAAtlAwAwAAAAAAAABsoWAGAAAAAAAAANhCwQwAAAAAAAAAsIWCGQAAAAAAAABgCwUzAAAAAAAAAMAWCmYAAAAAAAAAgC0UzAAAAAAAAAAAWyiYAQAAAAAAAAC2UDADAAAAAAAAAGyhYAYAAAAAAAAA2ELBDAAAAAAAAACwhYIZAAAAAAAAAGALBTMAAAAAAAAAwBYKZgAAAAAAAACALRTMAAAAAAAAAABbKJgBAAAAAAAAALZQMAMAAAAAAAAAbKFgBgAAAAAAAADYQsEMAAAAAAAAALCFghkAAAAAAAAAYAsFMwAAAAAAAADAFgpmAAAAAAAAAIAtFMwAAAAAAAAAAFsomAEAAAAAAAAAtvhkd0OHw5Gb48gkLS3NaJ4klS9f3mje9u3bjeZJUp06dYzmeXmZ/x1Gamqq0Twfn2x/GyGXmJ6fBg4caDRPkp555hmjedOmTTOaJ0nVqlUzmhcUFGQ0T5Jat25tNO/EiRNG85A103NUSkqK0TxJql69utG88ePHG82TpP379xvNO3bsmNE8yfy86OvrazQPmZk+Vw8ICDCaJ0nFixc3mrd48WKjeXmhSpUqxjNnzJhhNM/0v93wDHnRQxUtWtRoXmRkpNE8yfy8aFmW0TzJ/Pm3J/ZQXMEMAAAAAAAAALCFghkAAAAAAAAAYAsFMwAAAAAAAADAFgpmAAAAAAAAAIAtFMwAAAAAAAAAAFsomAEAAAAAAAAAtlAwAwAAAAAAAABsoWAGAAAAAAAAANhCwQwAAAAAAAAAsIWCGQAAAAAAAABgCwUzAAAAAAAAAMAWCmYAAAAAAAAAgC0UzAAAAAAAAAAAWyiYAQAAAAAAAAC2UDADAAAAAAAAAGyhYAYAAAAAAAAA2ELBDAAAAAAAAACwhYIZAAAAAAAAAGALBTMAAAAAAAAAwBYKZgAAAAAAAACALRTMAAAAAAAAAABbKJgBAAAAAAAAALZQMAMAAAAAAAAAbKFgBgAAAAAAAADYQsEMAAAAAAAAALCFghkAAAAAAAAAYAsFMwAAAAAAAADAFgpmAAAAAAAAAIAtPtnd0OFw5OY4MvHz8zOaJ0lBQUFG8+rXr280T5L27t1rNK9ChQpG8ySpbdu2RvNmzJhhNA+ZeXt7G8373//+ZzRPkgYNGmQ0LzAw0GieJI0dO9Zo3syZM43mSeb/nTl9+rTRPGTNy8vs7/MDAgKM5klSSkqK0bypU6cazZOkGjVqGM17/vnnjeZJ0rRp04zmmf73G5mZ/hkvL/7OV61aZTQvLi7OaJ4kNWrUyGjehg0bjOZJUmxsrNG8iIgIo3nImulzqLzooe677z6jeabP2SRp3rx5RvN8fLJddeaYxMREo3mmvzeyw/NGBAAAAAAAAAC4IVAwAwAAAAAAAABsoWAGAAAAAAAAANhCwQwAAAAAAAAAsIWCGQAAAAAAAABgCwUzAAAAAAAAAMAWCmYAAAAAAAAAgC0UzAAAAAAAAAAAWyiYAQAAAAAAAAC2UDADAAAAAAAAAGyhYAYAAAAAAAAA2ELBDAAAAAAAAACwhYIZAAAAAAAAAGALBTMAAAAAAAAAwBYKZgAAAAAAAACALRTMAAAAAAAAAABbKJgBAAAAAAAAALZQMAMAAAAAAAAAbKFgBgAAAAAAAADYQsEMAAAAAAAAALCFghkAAAAAAAAAYAsFMwAAAAAAAADAFgpmAAAAAAAAAIAtFMwAAAAAAAAAAFsomAEAAAAAAAAAtlAwAwAAAAAAAABsoWAGAAAAAAAAANjik90N09LScnMcmQQEBBjNk6TU1FSjecuWLTOaJ0mbNm0ymvfzzz8bzZOkXr16Gc0z/XWDzJKTk43mBQcHG82TpF9++cVo3oULF4zmSVKLFi2M5gUFBRnNkyRfX1+jeUlJSUbzkDXT/07kz5/faJ5k/vtp8eLFRvMkae7cuUbzxo8fbzRPkrp06WI0b//+/UbzkJnpn/FMn7NJUmJiotG8MmXKGM2TpEKFChnN++2334zmSVJ4eLjRPNPfG8ia6b+HEiVKGM2TpJo1axrNW7VqldE8yfwclZCQYDRPkkJCQozm5cW/p1fDFcwAAAAAAAAAAFsomAEAAAAAAAAAtlAwAwAAAAAAAABsoWAGAAAAAAAAANhCwQwAAAAAAAAAsIWCGQAAAAAAAABgCwUzAAAAAAAAAMAWCmYAAAAAAAAAgC0UzAAAAAAAAAAAWyiYAQAAAAAAAAC2UDADAAAAAAAAAGyhYAYAAAAAAAAA2ELBDAAAAAAAAACwhYIZAAAAAAAAAGALBTMAAAAAAAAAwBYKZgAAAAAAAACALRTMAAAAAAAAAABbKJgBAAAAAAAAALZQMAMAAAAAAAAAbKFgBgAAAAAAAADYQsEMAAAAAAAAALCFghkAAAAAAAAAYAsFMwAAAAAAAADAFgpmAAAAAAAAAIAtFMwAAAAAAAAAAFsomAEAAAAAAAAAtlAwAwAAAAAAAABsoWAGAAAAAAAAANjik90NvbzMdtEBAQFG8yRp9erVRvMKFSpkNE+SDhw4YDSvVatWRvMkqUyZMkbz+vbtazQPmXl7exvNCw4ONponScWLFzeaV7p0aaN5kvT1118bzfvqq6+M5knSp59+ajSvbt26RvOQNdPnUPny5TOaJ0nvvPOO0byHHnrIaJ4kFSxY0Gje8ePHjeZJUtGiRY3m7dy502geMjM9P/n4ZPvHzxxz/vx5o3mRkZFG8yTp4MGDxjNNM/21mpqaajQPWXM4HEbz+vXrZzRPkk6ePGk0b9u2bUbzJKlq1apG8xo0aGA0T5Luv/9+o3mDBw82mpcdXMEMAAAAAAAAALCFghkAAAAAAAAAYAsFMwAAAAAAAADAFgpmAAAAAAAAAIAtFMwAAAAAAAAAAFsomAEAAAAAAAAAtlAwAwAAAAAAAABsoWAGAAAAAAAAANhCwQwAAAAAAAAAsIWCGQAAAAAAAABgCwUzAAAAAAAAAMAWCmYAAAAAAAAAgC0UzAAAAAAAAAAAWyiYAQAAAAAAAAC2UDADAAAAAAAAAGyhYAYAAAAAAAAA2ELBDAAAAAAAAACwhYIZAAAAAAAAAGALBTMAAAAAAAAAwBYKZgAAAAAAAACALRTMAAAAAAAAAABbKJgBAAAAAAAAALZQMAMAAAAAAAAAbKFgBgAAAAAAAADYQsEMAAAAAAAAALCFghkAAAAAAAAAYAsFMwAAAAAAAADAFp/sbuhwOHJzHJn4+/sbzZOkn376yWhesWLFjOZJ0tChQ43m1ahRw2ieJI0ZM8Zo3rp164zmSdJXX31lPNOTmZ6f8uXLZzRPknbt2mU0b/369UbzJOnVV181mjd37lyjeZL5OdH09wayZvrvISAgwGieJNWsWdNo3uzZs43mSVJMTIzRvAMHDhjNk6QNGzYYzStatKjRPOQ9H59s//iZYyIiIoxnmrZ27VqjeeHh4UbzpLz5tw15z/Q5VMOGDY3mSdKaNWuM5vXo0cNoniSlp6cbzTtz5ozRPEl64403jOZ5e3sbzcsOrmAGAAAAAAAAANhCwQwAAAAAAAAAsIWCGQAAAAAAAABgCwUzAAAAAAAAAMAWCmYAAAAAAAAAgC0UzAAAAAAAAAAAWyiYAQAAAAAAAAC2UDADAAAAAAAAAGyhYAYAAAAAAAAA2ELBDAAAAAAAAACwhYIZAAAAAAAAAGALBTMAAAAAAAAAwBYKZgAAAAAAAACALRTMAAAAAAAAAABbKJgBAAAAAAAAALZQMAMAAAAAAAAAbKFgBgAAAAAAAADYQsEMAAAAAAAAALCFghkAAAAAAAAAYAsFMwAAAAAAAADAFgpmAAAAAAAAAIAtFMwAAAAAAAAAAFsomAEAAAAAAAAAtlAwAwAAAAAAAABsoWAGAAAAAAAAANhCwQwAAAAAAAAAsIWCGQAAAAAAAABgCwUzAAAAAAAAAMAWh2VZVl4PAgAAAAAAAABw4+EKZgAAAAAAAACALRTMAAAAAAAAAABbKJgBAAAAAAAAALZQMAMAAAAAAAAAbKFgBgAAAAAAAADYQsEMAAAAAAAAALCFghkAAAAAAAAAYAsFMwAAAAAAAADAFgpmGPHK0lfUbFKzvB4GAGQybuM4lf2wbF4PAwAy+fHPH+V4y5HXwwCALHEOBcBj/fij5OAcyiSfvB7Ajab15NZadXCVJCk1PVXpVrr8vP1cj+/uvVtlCpQxPq7JWyfrmYXP6Nl6z2rEHSOy/bxmk5pp9aHV8vFyfin4evsqslCk/nvbf9W5SufcGu5V/Xn6Tz0480EdiT+iY/2P5dk4gBuJp81PB2IP6KaPbpK/t7/b/UNaDFH/xv2v+vzuc7pr8rbJ8vXylSR5e3nrpgI3qU/9PupZt2eujDk7Y5qybYprzpSkAJ8Axb4SmyfjAW4UnjY/SdK249vU96e+2nh0o4L8gnRf1fv0bqt33cZ1JW/+/KYGrxzs2tbhcKhUSCl1r9ldA24dIG8v79wefpbGbRynD9Z+oOj4aFUoWEFvNXtLHSt3zJOxADcST5ujOIcC4NK6tbTKOT8pNVVKT5f8LjlX2b1bKmO4g9q2TerbV9q4UQoKku67T3r3XfdxXcmbb0qDB1/c1uGQSpWSuneXBgyQvPPgHMqynGOaOFGKiXG+nq+8InXtan4sNzAK5mu0uOti1/+/+fOb+vHPH7X2ibV5OCLp2QXPasPRDSodWtrW8/s37u8qpS+kXtDs32frwZkP6ufuP6txqcY5OdRsWb5/ubp+31WNSjbSkfgjxvOBG5Unzk+SlDQoyfZz7696v6bfN12S8we+FftXqPO3nRUaEKoHqz+YU0O8JoNuH6Q3m72ZJ9nAjcrT5qeE5AS1mdJGj9V8TAv+s0D7Y/er7TdtVThfYQ26fVC29lE/or7rGNKtdG08ulGdZ3SWl8NLrzR5JTeHn6VZUbP0ytJXtOA/C1Q/or6+3vq1Hpj5gH5/9neVCytnfDzAjcTT5qgMnEMB0OKL85PefNN5Ze7aPJyfEhKkNm2kxx6TFiyQ9u+X2raVCheWBmXvHEr16188hvR0Z1HdubPk5eUsdk376CPp66+dr3WFCtL330tdukjVq0u1apkfzw2KJTJygeMthz747QMVH1VcI1aP0KQtk1RsZDG3bRpOaKg3f37TdXv0+tGq8mkV5RuaT9XGVNPcXXNdjw1ZNURNJzW9Yl7p0NL6pccvCs8Xft1j9/fx10M3P6SmZZtqzq45kpy/bX5i3hNqNqmZqo+pLkk6ff60Hpn9iIqPKq7g4cHqOL2jouOjXfuZv3u+IkdHKmhYkLrM7KLElETXYwdjDypgSID2xOzJcgwxiTFa2nWpOlTqcN3HA8Cd6fkpJ/l4+ahV+VZ6sNqDmv37bEnOHwI7TO2gLjO7KGR4iCTpfMp59V7YW6U/KK38w/Kr+VfNFXUyyrWfdUfW6ZZxtyj/sPxqNbmVTpw74ZYTMCRAS/YuMXJMAC4yOT8dTziuthXa6q3mb8nfx1+VC1fWvVXudV3BeK28HF6qH1Ffver2cs1Pk7ZMUvUx1dXvp37KPyy/jp49qnQrXW+seEPlPy6vfEPzqd7n9fTroV9d+/kj5g/dOvFWBQ0LUoMJDfRHzB9uOZGjIzXhfxOyHMP51PMa3nK4bi19q3y9ffV47ccV7BestUfyviQD/gk4h+IcCvBYDof0wQdS8eLSiBHSpElSMff5SQ0bOgvqDKNHS1WqSPnySdWqSXMvzk8aMkRqeoX56fhxZ6H81luSv79UubJ0770Xr7K+Vl5ezsK5Vy9ptnN+0qRJznK3Xz8pf37p6FFnEf3GG1L58s4x16sn/XrxHEp//CHdeqvziuoGDZy3LxUZKU3I+hxKt9wiTZ3q3Mbb23lFdmioFBWV9fbIEgVzLpmze4629NyiAbcOuOq2s3+frbdWvqUp90xR/Kvxerv523pg5gM6FHdIkvM3vSu7r7zi8wc0GSB/H/8rPm5HWnqavB0X35owd/dc9W/cX9t7bZfkLJ0TUxIV9UyUovtGK8gvSD3m9pAkxSbFqsvMLupdr7dODzit7rd019dbv3btq0yBMkoalKRKhSplmX1/tftVJbxKjh4PgItMzk+S1O37bio+qrjC3wvXq0tfVUpaynWNP81Kc3v7+doja9WsTDOdGXBGkjRg6QBtPrZZa59Yq1MvnVK9EvXUeUZnWZaltPQ03ffdfWpTvo1iXo7RkOZDNH7TeLf9Jw1KUqvyra6Yv3z/ctX6rJaChwer/uf1tenopus6HgAXmZqfyhcsr4kdJ7q9Vftw/GFFhERc1/gvn5+Onj2qQN9AxQ6IVYngEvpw7YeatmOafnz4R8W+EqtuNbrprml36VzyOUnSo3MeVZnQMjre/7i+6vSVPtv0mdv+d/ferSdqP5Fl9iM1HlGver1ct2OTYnU2+awigq/vmABcxDkU51CAx5ozR9qyxbnMxNXMnu0siKdMkeLjpbfflh54QDrknJ80aJC08grzU/nyzqUkfC5ZEOHwYSniOs830tLcl8c4elQKDJRiY6USJaQPP5SmTXNewR0bK3XrJt11l3TOeQ6lRx91Lm1x/Lj01VfSZ+7nUNq9W3oi63MoNW/uLKUl6fx5Z/nu7S21bHl9x/QvQ8GcSx6o+oCKBhWVIxuLin+x+Qs9Xutx1SlRRz5ePupcpbOalG6iadunGRipu6TUJE3dPlWrD63WvVXvdd1ftkBZdajUQQ6HQyfOndD8PfM1rOUwhQWGKcQ/RCNajtCSfUt0LOGYfvrzJwX5BenZ+s/Kz9tPbSu21W1lbjN+LACyZmp+8vf2V+NSjXVP5Xt06IVDWvCfBZqyfYreXvW2rXGnpKVoyd4l+nbnt+pSrYvrfm8vbz1d92l5e3kr3UrXpC2T9Nrtr6lEcAkF+gZqSIshOhh3UOuj12vj0Y06evaoBt42UAE+AWpQsoHuqXxPtsdQPqy8KhasqAX/WaDovtG6rfRtajW5lWISY2wdEwB3eXX+NG/3PM3fPV/9G119bdOspKWnad2Rdfps02du81PchTi9fOvL8vX2dY25b6O+qlioovy8/dSnQR+FBYbphz0/6FjCMf125De92uRV5ffLr8qFK6tHzR62xmNZlp6c/6QaRDRQ07JmrpAE/g04h+IcCvBYDzwgFS2avQ+2++IL6fHHpTp1nEVx585SkybOAvdazZsnzZ8v9bd3DqW0NGndOmch3OXi/KS4OOnllyVf34tj7ttXqljRuX5znz5SWJj0ww/SsWPSb79Jr77qvOK5cmWph41zqCefdD5/1ChnYX/5VeD4W6zBnEuu5UMg9p7eq8V7F+vDtR+67ku30lW1cNVcGFlmI9eMdGX7efupanhVzX1wruqWqOvapkzoxePZd2afJKnmuJpu+/F2eOtw3GEdiT+i0qGl5eW4+PuLSgUradNf/IYa8ASm5qfiwcX162MX37ZUP6K+/tvkvxq2epgGNx+crfzvor7TnCFzJDnf3lmxUEWNaT9GnSp3cm1TKqSU6we9E+dO6GzyWXWc3lEOXTy5SrPSdDj+sBxyKCwgTKEBoa7HrvRuiqy81vQ1t9vvtnpX03ZM05xdc/R47cezvR8AWcuL86fZv8/Wo3Me1eR7JqtakWrZft766PUKGBIgyblERtkCZdW3YV891+A51zZhAc5fxF865ucWPacXfnzBdV/G/JSx1NhNYTe5HruW+SlDSlqKus/trp0ndmrFoyuu+fkAroxzKM6hAI91LR/0t3evc73hDz+8eF96ulT1Gjuo2bOdVw5PnuxcZiO71q+XApznUPLyksqWdZbHz108h1JYmBRy8RxKe/c6H3/hhYv3paU5r56O/v/lWm+6eA6lStd+DqXPP5c+/liaPl3q0EFavpw1mK8BBXMuufQtl1lJs9Jc/x/oG6gRLUeoX+N+uT2sLF36IX9XcunxBPoESpKi+0arUL5CmbZdsm+JUtNT3e5Lt9JzYKQAckJezk9lC5TVsYRjsiwrW1f/XPoBNVeS1fy05rE1qlOiTqZtp26fmqPzk7eXt0qFltLRs0dt7wPARabnp/GbxmvA0gGa9cAstS7f+pqee+mH/F3J5ccT6BuoCXdNcHuXWIY1h9dIktscda3z0/mU8+o4vaMSUxL1S49fsjxPA2Af51CcQwEey+cq9V7axflJgYHOtZr7Xcf8NH68czmOWbOk1td2DuX2IX9XcvnxBAY611C+N/M5lNY4z6GUeskclW5zfgoMdF79PH2686rp0aPt7edfiCUyDAjwCXD7kLu09DQdiD3gul0+rLy2ndjm9pxDcYdkWZapIV6TsgXKysvhpW3HL445JS3FdXJQIriEos9Gu40/6hSLowOeKDfnp2X7lmnoqqFu9/1+6neVLVA2Wz8Y2REaEKpCgYXc5idJrmMqEVxC8RfiFZcU53rs0g+v+TuWZanvT33d9p2clqy9p/eqXFi56x88ADe5ff40M2qmBi4fqBWPrrjmctmu8mHl/3Z+kqTDcYddj2V3fpKcc9SDsx6Ur7evlnZbSrkM5DLOoTiHAjxWQICUeHF+UlqadODAxdvly0vb3L/XdeiQlN0OauZMaeBAacWKay+X7cpqzBnHVMJ5DqXDF8+hrukD+u66S/r0U/f7vLwuLs+BbKFgNqBiwYo6m3xWi/cuVnJasoavHu52YtGzTk/N2DFDC/YsUGp6qlbsX6HqY6prXfS6685eH71elUdXVnJa8nXvK0NoQKgerP6gBiwdoCPxR3Q+5bxeXfaqWk1uJcuydEe5OxSXFKfPNn2m5LRkzd01V+uOXP+xAMh5uTk/FQgo4Pxwm21TlJKWoo1HN2rkmpHqVdf5IVTR8dGqPLqy9p/Zn6PH1LNOTw35ZYh2ndqllLQUffDbB6r3eT0lpiSqQUQDhQWG6d1f39WF1AtafWi1fvjjh2zt1+FwaH/sfj2z4BlFx0crITlBA5YMkK+3r9vbTQHkjNycn+KS4tRrQS9NuWeKaharmeU2lUdX1upDq3PqcFxj/nTDp1p7ZK3S0tP07c5vVW1MNR2KO6SyBcqqSuEqGvnbSCWmJGrHiR2avG1ytvc9dftU7TyxU9/d/50CfAJydNwAMuMcinMowGNVrCidPetcBiM5WRo+3L087tlTmjFDWrDAedXvihVS9erOtZCvJi5O6tXL+QGBNWtmvU3lytLqnD2HUs+ezhJ47VpnYf7tt85lOQ4dci6xUaWKNHKks1jfscO5bEd2NWnivKJ782bn6zF/vrR0qbN4RrZRMBtQp0QdvdjwRXWZ2UUR70fI18tXjUs1dj3eqnwrjWw9Ur0X9Vbw8GA9u/BZjW0/Vg1LNpQkDVk1RE0nZf0BLQdjDypgSIAChgRo1cFVGrlmpAKGBChydKQkKTElUbtjduf4MX3S9hNVKFhB1cZUU4n3SyjqZJTmPjhXDodDJUNKatq90zRyzUiFvROmKdun6Jl6z2Qa856YPVnuu/Xk1goYEqAn5z+p4+eOux0fgJyVm/NTnRJ1NOO+GRq5ZqRCR4Tq7ml3q0/9Pnqh4QuSpJT0FO2O2a2U9Ov7RPTLvdb0Nd1Z/k41mdhEhd4tpO93fa9FDy9SPt98CvQN1JwuczR391yFvROmN39+U/0aub81LGBIgJbsXZLlvr+4+wtVLFRRdcbXUZH3imjL8S1a8egK5ffLn6PHACB356d5u+fpVOIpdZze0XWekfEnw+6Y3W5XJ+aEx2s/rmfqPaPOMzorZESI3vn1HX3f5XuVDi0tSZr5wEztOrVL4e+Fq8fcHnqp8Utuz48cHakJ/5uQ5b4nbpmoA7EHVPCdgm7H8+S8J3P0GAA4cQ7FORTgserUkV580fmheRERzitxG1+cn9SqlbOM7d1bCg6Wnn1WGjtWauicnzRkiNT0Ch8SPG+edOqU1LGj80rpS/9k2L3b/QrqnPD449Izzzg/kDAkRHrnHen776XSznMozZwp7dolhYc7l7h4yf0cSpGRziU2stK/v/TUU1L79s59v/KKc9sWLXL2GP7hHJanrsOAHNP2m7Za9PCivB4GAGTS7ftuGtl6pIrkL5LXQwEAN6+veF0dKnVQ/Yj6eT0UAMiEcygAHuv1150fklefc6h/E65g/oc7lnBMft5+eT0MAMgkKTVJB2IP8IMRAI+08uBK3VL0lrweBgBkwjkUAI+2cqV0C+dQ/zZcwQwAAAAAAAAAsIUrmAEAAAAAAAAAtlAwAwAAAAAAAABsoWAGAAAAAAAAANhCweyhYpNiVf7j8lq6b2leDyVLlmWp1eRWGv7L8LweCgADPH1OupoLqRdUfUx1Tds+La+HAiCXefp8xXwE/Ht4+nx0NcxXwL9MbKxUvry01IPnrCFDpLZtJT5OzuNQMHuoXgt66c7yd+qOcndo2vZpqjG2hvIPy69qY6pp8d7Fru3OXjir3gt7q+T7JRU0LEidZ3TWqcRTWe5zyKohChgS4PbH921fNf+quSRp1cFVKvdRORV6t5DGbBjj9tyDsQdV+oPSOnnupCTJ4XDoy45f6p1f39Gmo5ty6VUA4CkunZMsy9LINSPl97afxm0c57ZdupWugcsGqtxH5RT2TpjunHKn9p3Z53r89PnT6jKzi4qOLKrio4rriXlP6HzK+SvmztgxQzXG1lDw8GDVGV/Hbf6bGTVTxUcVV/FRxfX979+7PW999HpVHl1ZSalJkiR/H3991ekr9VrQS4fjDufESwLAQ+XGOZSkv90X8xGArHD+BOCG0quXdOed0h13SPHx0qOPSiEhUliY9NRT0vks5p3oaCk4WHrzzexlvPii5HBcvP3XX9Kttzr38eij7sVxaqpUq5a0bNnF+159VTpxQvr4Y1uHiFxkweNsO7bN8nvbzzocd9haeWCl5TPYx5odNdu6kHrBmrtrrhUyPMQ6GHvQsizLemzOY1bNcTWtvaf3WvFJ8VaPOT2sdt+0y3ZW68mtrTHrx1iWZVl1x9e15vw+xzoaf9Qq9E4h68z5M67tOkztYE3838RMz++zsI9119S7ru+AAXi0S+cky7Ksdt+0s9pOaWsVea+INXbDWLdtP177sVX2w7JW1IkoKz4p3uq9oLdVY2wNKz093bIsy+o8o7PV/pv21slzJ63o+Gir8ReNrT4L+2SZu/mvzZb/2/7Wgj0LrPMp560pW6dY+Ybmsw7HHbbS0tOsou8VtTb/tdna8tcWq8SoEq6MlLQUq+a4mtayfcsy7fOuqXddMQ/AjS+3zqH+bl/MRwCywvkTgBvKtm2W5ednWYedc5Z1333OP6dOOe9r1cqyJk/O/LzOnS0rNNSy3njj6hmbN1tWwYKWdWkV2b+/Zb34omUlJVlWgwaW9eOPFx977z3L6tYt835mzbKsIkUs6/z5azhA5DauYPZAYzeOVZvybVQypKTm756vpmWa6p4q98jP2093R96tNuXb6Jtt30iS5u2Zp36N+qlcWDkF+wfrozs/0k9//qSjZ49eNWdm1EwdSzimp+o8JUnadnyb2lRoo+LBxVUurJx2ndolSZoVNUsJyQnqUatHpn30rNNTP+z5QdHx0Tn4CgDwJJfOSZLUqGQjLfjPAgX6BGba9rNNn+nFhi+qSngVBfsHa1jLYYo6GaV10et0POG45uyao2Eth6lwvsIqEVxCr93+mr7c8qVS0lIy7WvC/yaoXcV2alexnQJ8AvRwjYd1c5GbNWXbFB1POC5Jqlmspm4pdotS0lJ0/Jzzvo/WfqRbit6iFje1yLTPnnV6auLmiUpOS87JlwiAh8itc6i/2xfzEYCscP4E4IYydqzUpo1UsqR08KA0d640erRUqJDzvsWLpUcecX/OwoVSVJTUocPV95+eLj39tNS3r/v927ZJrVtL/v7S7bdLmzc77z90yJk/alTmfXXq5Pzv7NnXfJjIPRTMHmjZ/mVu/7A7Ln37gKSwgDBtOb7l4uO6+Hg+33zy8/bT1mNb/zYjLT1NA5YO0PCWw+Xt5e3aT7qVLkmyZMkhh+IvxOvlpS+rX6N+uuPrO9RgQgNN3DzRtZ9qRaqpcL7CWnFghe3jBeDZLp+TBt0+KNO8JEnnU84r6mSUahev7bov2D9YFQtW1IboDdpybIu8Hd66ucjNrsdrF6+thOQE1y+0LrXpr01u+8rYfsPRDXI4Ls5X0sU561DcIX2y/hPdV/U+3fblbWr0RSMt2LPAtd1tZW5TUmqS1kevt/diAPBouXkOdaV9MR8ByArnTwBuKMuWSS3+f85avVoqXVqaPFkqUUKKiJBeecW5ZEWG8+el3r2lMWMkH5+r7/+zz6SAAOnhh93vdzic5bPkXB4jY57s3VsaMEB64QWpbl3puecuLp/h5eUso5cvv65DRs6iYPYwKWkp2hOzx3UC0aFSB63Yv0Jzd81VclqyVh1cpfl75uv0+dOux99b854OxB7QueRzeuPnN2TJcj1+JdN2TFOIf4jaVWznuq928dr6Yc8P2n9mvw7EHlDV8KoatHyQHr3lUY3bOE7da3bXkq5L9PqK13Xi3AnX86oVqaYdJ3bkwqsBIK9dPif9nTNJZ2TJUlhAmNv9BQML6lTiKcWcj1FoQKjbD1cFAwtKUpbrnsYkxlxxX0XzF5Wft5/WHVmnNYfXKMgvSEWDiqr3wt4a3HywXln6ioa3HK5v7/tWT85/0nWFT4h/iEqFlmLOAv6BcvMc6u/2xXwE4HKcPwG4oaSkSHv2SDf//5x15IhzbeXDh533z54tffGF84riDIMHS40aSc2bX33/x49Lb7zhLKMvV7u280rohATnhws2aODMO3fO+ScgQNq40TmOuXMvPq96dWkHc5InoWD2MBk/1GScNDQt21SftvtULy15SeHvhWv0+tHqdks3+Xg5f0P0fuv3VaNoDdX7vJ6qfFpF4fnCVS6snOvxK/lw7Yd6rv5zbve93+Z9DVw+UA0mNNC7d7yr3TG7teLACr3S5BWtObxGd0ferRD/ENWPqK91R9a5nlc4X2HXh/8B+Ge5fE7KDktX/kRf6xo/7fdK+3I4HBrTfozu/fZedZnZRWPajdHs32crMSVRHSM76ujZo2pSuolKhZZSsaBiblf4MGcB/0y5eQ71d/tiPgJwOc6fANxQTv//L9cL/v+cZVnOq5XffVcKCnKWvk88IX37rfPxqChpwoSsl6/ISt++Uo8eUtWqmR978UVp+3bnMhwNG0p16jivXB43TlqzRrr7bud27dpJv/xy8XmFC0snmZM8STauY0deuPQ31D3r9lTPuj1dt/ss7KOI4AhJUlhgmL6+52vXY5Zl6bUVrykiJOKK+95/Zr82H9usDpXc18lpWLKh/ujzhyTnEhoNJjTQ2PZj5eftp7gLcQryC5Ik5ffLr7gLcRfHKsffnhABuPFl9ZbOyxUMLCgvh5diEmPc7o85H6Mi+YsoPF+44i7EKS09zbU0T8a2RfIXybS/8PzhmfeVGOPa9u7Iu3V3pPOE4+yFs6o9vrYWPbxI8RfiXfOVxJwF/Nvk1jnU3+2L+QhAVjh/AnBDyZizihWTAgOd6yJnKFtWmjHDWT736iW9+aZzu6tZtsxZFF/pauPwcGnlyou3n3tO6t5dqlhRiotzFtySlD+/8/alY73GX74hd3EFs4fJ+C13xknBkfgjmrZ9mts2S/YtUeNSjSVJqw6uclsLa+2RtUpNT1WtYrWumDF391zVLFZT4fnDr7jNx+s+Vu3itdWkdBNJzrdFnTl/xjW2YL9g17YnE08qPN+V9wXgxnX5nPR3AnwCVL1IdW36a5PrvtikWP15+k81KNlAtYrXkmVZ2nr84vqmG45uUIGAAoosHJlpf3WL13XbV8b2DSIaZNp20PJB6lGzhyoUrKAQ/xDFJsW6HmPOAv4dcvMc6mr7uhTzEQDOnwDcUDKuXI75/zmralXp7Flp376L2xw4IJUp4/zwvVWrnEteFC7s/DN9uvNq59q1M+1aU6Y4l8goU8a5bcY2Gc+71MaN0s8/Sy+/7LwdEiKdOXNxbMEX5ySdPOksp+ExKJg9jK+3ryoVquRa3yopNUnd5nTT/N3zlZqeqqGrhupcyjl1qdZFkrR8/3L1mNtDxxOO68S5E3rhpxf0dN2nld8vvySp2/fd9P5v77tlbD62WTcVuOmKYzgcd1ifbvhU79zxjuu+hiUb6ruo73T07FGtj16v+hH1XY9FnYzSzUWvvr4YgBvP5XPS1fSq20sfrftIu07t0tkLZzVgyQDVKlZLdUvUVeF8hXVf1fs0aPkgnUo8pSPxRzR45WA9UesJ11vSW37dUjN2zJAkPVnnSS3Zt0QL9ixQUmqSJm6eqD0xe/RIDfdPL950dJN+PvizXmr8kiQpNCBUESER+vHPH7X9+HYdP3dcVcKrSHJeqXM47jBzFvAPlJvnUFfbVwbmIwAS508AbjC+vlKlShevMq5Xz7lUxQsvSLGx0pYtzjWYe/RwLmVx+LDzvow/d98tPf20cy1lyblW84MPOv///fed6ydnbJuxTcbzMqSlOa+MHjvWOR7JuWTG7NnODxScN09qfMkv9nfuvLhmNDwCBbMHanlTSy0/4Pw0zAoFK+iLu79Qn0V9FDI8RD/u/VE/Pvyj64efV5q8otrFa6vS6Eqq8mkV1S9RXyPuGOHa16G4Q24fyCdJxxKOqVjQld/K0GdRHw1pMURhgRc/HGJkq5H6ZP0nqjG2hoa0GKLiwcUlOcvlk+dOqnnZbCzsDuCGdOmctOrgKgUMCVDAkAAdjDuoPov6KGBIgFpPbi1J6lmnp7rf0l1NJzVV0ZFFdeTsEc3uMtu1r886fKbQgFDd9NFNqjG2hupH1NfQlkNdj+89vVdnkpy/pa5epLq+6fyNXvzpRYWOCNUn6z/RD//5wW3+SktP09MLntbY9mPl6+3run9c+3Hq+UNPtZnSRhPvnig/bz9J0i+HflGAT4DbL8kA/HPk1jnU1fYlMR8BcMf5E4AbSsuW0nLnnCWHQ/r+e+c6zBERUps2Uv/+Uteukre3s2S+9E++fM6rjTOWzDh1ynnFsySFhblvm7FNxvMyfPKJs9S+9daL9/Xq5VwfulgxKTJSuvde5/2W5byKukWLXH1JcG0c1rV+YgBy3bbj21Tv83ra99y+v11L2RO88OML2ndmn+Y9NC+vhwIgl9xIc9LVdJreSaVDS+vjth/n9VAA5IIbab5iPgL+2W6k+ehqmK+Af4Ft25xXLu/b5yyVPdmcOVLPntLBg1JAQF6PBv+PK5g9UI2iNdS5SmeNWD3i6hvnoej4aH219Su90fSNvB4KgFx0o8xJV7P5r81aeXCl662gAP55bpT5ivkI+Oe7Ueajq2G+Av4latSQOneWRnj4nJWWJg0dKv33v5TLHoaC2UONbT9WC/9cqGX7luX1ULJkWZZ6zO2hlxu/rDol6uT1cADkMk+fk67mQuoFdZvTTWPajVGp0FJ5PRwAucjT5yvmI+Dfw9Pno6thvgL+ZcaOda6RvMyD56wRI6RChaTnnsvrkeAyLJEBAAAAAAAAALCFK5gBAAAAAAAAALZQMAMAAAAAAAAAbKFgBgAAAAAAAADY4pPdDbt3756Lw8gsISHBaJ4kdejQwWje3LlzjeZJ0pw5c4zmNWnSxGieJIWHhxvNy5cvn9E8SZoyZYrxTE/22GOPGc07ceKE0TxJuv32243m/fHHH0bzJKlp06ZG8xo2bGg0T5Juuukmo3ldu3Y1midJU6dONZ7p6Xr16mU079ixY0bzJMnPz89o3gMPPGA0T3J+wLFJefG9lJycbDSvcOHCRvMkadKkScYzPdl7771nNK969epG8ySpUqVKRvOOHj1qNE8yPz/VrFnTaJ4krVmzxmjehg0bjOZJ0muvvWY809M1a9bMaF6fPn2M5knS+vXrjebt3bvXaJ4k7dixw2heWlqa0TxJKlCggNG8gIAAo3mS9Msvv/zt41zBDAAAAAAAAACwhYIZAAAAAAAAAGALBTMAAAAAAAAAwBYKZgAAAAAAAACALRTMAAAAAAAAAABbKJgBAAAAAAAAALZQMAMAAAAAAAAAbKFgBgAAAAAAAADYQsEMAAAAAAAAALCFghkAAAAAAAAAYAsFMwAAAAAAAADAFgpmAAAAAAAAAIAtFMwAAAAAAAAAAFsomAEAAAAAAAAAtlAwAwAAAAAAAABsoWAGAAAAAAAAANhCwQwAAAAAAAAAsIWCGQAAAAAAAABgCwUzAAAAAAAAAMAWCmYAAAAAAAAAgC0UzAAAAAAAAAAAWyiYAQAAAAAAAAC2UDADAAAAAAAAAGyhYAYAAAAAAAAA2ELBDAAAAAAAAACwhYIZAAAAAAAAAGCLT3Y3TE9Pz81xZJKYmGg0T5L27t1rNK9YsWJG8yQpPDzcaF5CQoLRPEkKCwsznom8lZqaajSvefPmRvMkKSIiwmheyZIljeZJ0uHDh43mvf/++0bzJCklJcVonmVZRvOQNdNz1KlTp4zmSVKzZs2M5n333XdG8yTp4MGDRvM6depkNE+Sli5dajwTecv0/JSUlGQ0T5KOHz9uNC8v5qetW7cazfP39zeaJ0kNGjQwmufn52c0D1kzfS7766+/Gs2TpBMnThjNy4uv7YYNGxrNW7dundE8yXxn6om4ghkAAAAAAAAAYAsFMwAAAAAAAADAFgpmAAAAAAAAAIAtFMwAAAAAAAAAAFsomAEAAAAAAAAAtlAwAwAAAAAAAABsoWAGAAAAAAAAANhCwQwAAAAAAAAAsIWCGQAAAAAAAABgCwUzAAAAAAAAAMAWCmYAAAAAAAAAgC0UzAAAAAAAAAAAWyiYAQAAAAAAAAC2UDADAAAAAAAAAGyhYAYAAAAAAAAA2ELBDAAAAAAAAACwhYIZAAAAAAAAAGALBTMAAAAAAAAAwBYKZgAAAAAAAACALRTMAAAAAAAAAABbKJgBAAAAAAAAALZQMAMAAAAAAAAAbKFgBgAAAAAAAADYQsEMAAAAAAAAALCFghkAAAAAAAAAYAsFMwAAAAAAAADAFgpmAAAAAAAAAIAtFMwAAAAAAAAAAFt88noAV9KzZ0/jmdu3bzead/PNNxvNk6Tdu3cbzYuPjzeaJ0mWZRnPRN5yOBxG8/r162c0T5I++eQTo3mzZs0ymidJO3fuNJqXlJRkNE+SatasaTSvVKlSRvOQNdNzVF58bR88eNBonun5QpIqVKhgNC80NNRoniSlpaUZzTP9vYHMTP8dFCxY0GieJB05csRo3u+//240T5ICAwON5iUnJxvNk6SYmBijeSVKlDCaB8+watUq45n79u0zmleoUCGjeZJ02223Gc3Lnz+/0TzJ/DmUJ+IKZgAAAAAAAACALRTMAAAAAAAAAABbKJgBAAAAAAAAALZQMAMAAAAAAAAAbKFgBgAAAAAAAADYQsEMAAAAAAAAALCFghkAAAAAAAAAYAsFMwAAAAAAAADAFgpmAAAAAAAAAIAtFMwAAAAAAAAAAFsomAEAAAAAAAAAtlAwAwAAAAAAAABsoWAGAAAAAAAAANhCwQwAAAAAAAAAsIWCGQAAAAAAAABgCwUzAAAAAAAAAMAWCmYAAAAAAAAAgC0UzAAAAAAAAAAAWyiYAQAAAAAAAAC2UDADAAAAAAAAAGyhYAYAAAAAAAAA2ELBDAAAAAAAAACwhYIZAAAAAAAAAGALBTMAAAAAAAAAwBYKZgAAAAAAAACALRTMAAAAAAAAAABbKJgBAAAAAAAAALb4ZHdDh8ORm+PIZMGCBUbzJMmyLKN5fn5+RvMkycvrn/87hbS0NKN5/4bX1NOZnp9++eUXo3mSVKBAAaN5NWvWNJonSStXrjSalz9/fqN5kpSSkmI0z9fX12gePENqaqrxzG+++cZoXnp6utE8SYqIiDCal5ycbDRPMj9H+fhk+0cR5BLT51D58uUzmidJN910k9G8J5980mieJM2bN89o3pEjR4zmSeb/bWN+8gym56jz588bzZMkf39/o3lnz541midJhw8fNpqXFz8DmZ6jPLGH8rwRAQAAAAAAAABuCBTMAAAAAAAAAABbKJgBAAAAAAAAALZQMAMAAAAAAAAAbKFgBgAAAAAAAADYQsEMAAAAAAAAALCFghkAAAAAAAAAYAsFMwAAAAAAAADAFgpmAAAAAAAAAIAtFMwAAAAAAAAAAFsomAEAAAAAAAAAtlAwAwAAAAAAAABsoWAGAAAAAAAAANhCwQwAAAAAAAAAsIWCGQAAAAAAAABgCwUzAAAAAAAAAMAWCmYAAAAAAAAAgC0UzAAAAAAAAAAAWyiYAQAAAAAAAAC2UDADAAAAAAAAAGyhYAYAAAAAAAAA2ELBDAAAAAAAAACwhYIZAAAAAAAAAGALBTMAAAAAAAAAwBYKZgAAAAAAAACALRTMAAAAAAAAAABbKJgBAAAAAAAAALZQMAMAAAAAAAAAbPHJ7oYOhyM3x5HJ4cOHjeZJ0qlTp4zmhYeHG82TpBIlShjNO378uNE8SUpOTjaa5+XF72nymum/g759+xrNk6SHH37YaN5tt91mNE+SwsLCjOaNGDHCaJ4knT171miet7e30TxkzfTfQ0BAgNE8yfz3b2xsrNE8SWrWrJnRvIULFxrNk8zPUT4+2f5RBLnE9DlUcHCw0TxJ2rp1q9E8099HkvTII48YzXvttdeM5klSQkKC0TzOof6d8uJne39//390nmT+PPHPP/80mieZP/823dFmB80YAAAAAAAAAMAWCmYAAAAAAAAAgC0UzAAAAAAAAAAAWyiYAQAAAAAAAAC2UDADAAAAAAAAAGyhYAYAAAAAAAAA2ELBDAAAAAAAAACwhYIZAAAAAAAAAGALBTMAAAAAAAAAwBYKZgAAAAAAAACALRTMAAAAAAAAAABbKJgBAAAAAAAAALZQMAMAAAAAAAAAbKFgBgAAAAAAAADYQsEMAAAAAAAAALCFghkAAAAAAAAAYAsFMwAAAAAAAADAFgpmAAAAAAAAAIAtFMwAAAAAAAAAAFsomAEAAAAAAAAAtlAwAwAAAAAAAABsoWAGAAAAAAAAANhCwQwAAAAAAAAAsIWCGQAAAAAAAABgCwUzAAAAAAAAAMAWCmYAAAAAAAAAgC0UzAAAAAAAAAAAW3yyu2F6enpujiMTX19fo3mS5O/vbzTv3LlzRvMkqWLFikbzNm/ebDQvL6SlpeX1EP71UlNTjeYFBAQYzZOk9957z2hepUqVjOZJ0sCBA43mRUZGGs2TzH+tJicnG81D1lJSUozm5c+f32ieJAUFBRnNK1mypNE8SWrSpInRvBEjRhjNk6SbbrrJaN6FCxeM5iEz0/8ujR492mieJB06dMhoXtmyZY3mSdKtt95qNC88PNxoniR5eZm9No75yTNYlmU0z8cn2xVZjjH9/VS9enWjeZLk7e1tNK9Ro0ZG8yQpMTHRaF5MTIzRvOzgCmYAAAAAAAAAgC0UzAAAAAAAAAAAWyiYAQAAAAAAAAC2UDADAAAAAAAAAGyhYAYAAAAAAAAA2ELBDAAAAAAAAACwhYIZAAAAAAAAAGALBTMAAAAAAAAAwBYKZgAAAAAAAACALRTMAAAAAAAAAABbKJgBAAAAAAAAALZQMAMAAAAAAAAAbKFgBgAAAAAAAADYQsEMAAAAAAAAALCFghkAAAAAAAAAYAsFMwAAAAAAAADAFgpmAAAAAAAAAIAtFMwAAAAAAAAAAFsomAEAAAAAAAAAtlAwAwAAAAAAAABsoWAGAAAAAAAAANhCwQwAAAAAAAAAsIWCGQAAAAAAAABgCwUzAAAAAAAAAMAWCmYAAAAAAAAAgC0UzAAAAAAAAAAAWyiYAQAAAAAAAAC2UDADAAAAAAAAAGzxye6GDocjN8eRiY9PtoeWY26++WajeaGhoUbzJCkgIMBoXrNmzYzmSZK3t7fRvOPHjxvNQ2ZeXmZ/V5YvXz6jeZJUpUoVo3kVKlQwmidJycnJRvPi4uKM5klSSEiI0bykpCSjecia6TkqMDDQaJ4kXbhwwWjeW2+9ZTRPkk6fPm00r2fPnkbzJGnDhg1G85ij8p7pn/H8/PyM5knmf65s2bKl0TxJWrx4sdG8mJgYo3mSFB4ebjTP9HkpPEP37t2NZ+7atctoXl7822v6PDE2NtZoniTFx8cbzbMsy2hednAFMwAAAAAAAADAFgpmAAAAAAAAAIAtFMwAAAAAAAAAAFsomAEAAAAAAAAAtlAwAwAAAAAAAABsoWAGAAAAAAAAANhCwQwAAAAAAAAAsIWCGQAAAAAAAABgCwUzAAAAAAAAAMAWCmYAAAAAAAAAgC0UzAAAAAAAAAAAWyiYAQAAAAAAAAC2UDADAAAAAAAAAGyhYAYAAAAAAAAA2ELBDAAAAAAAAACwhYIZAAAAAAAAAGALBTMAAAAAAAAAwBYKZgAAAAAAAACALRTMAAAAAAAAAABbKJgBAAAAAAAAALZQMAMAAAAAAAAAbKFgBgAAAAAAAADYQsEMAAAAAAAAALCFghkAAAAAAAAAYAsFMwAAAAAAAADAFgpmAAAAAAAAAIAtFMwAAAAAAAAAAFt88noAV+LjY35o5cuXN5p39uxZo3mSFB8fbzSvSpUqRvMkqVmzZkbzqlatajQPmTkcDqN5/v7+RvMkqUWLFkbzQkNDjeZJ0ujRo43mJSQkGM2TpDJlyhjNM/29gaz9G+aoO++802heYmKi0TxJGjlypNG8p59+2mieJL3zzjtG8/7zn/8YzUNmpuenwYMHG82TpKCgIKN5gwYNMponSUOHDjWa16ZNG6N5khQSEmI0j3Moz2D67+H55583midJffr0MZq3a9cuo3mStHbtWqN5ZcuWNZonSQULFjSaFxAQYDQvO7iCGQAAAAAAAABgCwUzAAAAAAAAAMAWCmYAAAAAAAAAgC0UzAAAAAAAAAAAWyiYAQAAAAAAAAC2UDADAAAAAAAAAGyhYAYAAAAAAAAA2ELB/H/t2rEJADAAwzD6/9HpD15KQbogswkAAAAAAInADAAAAABAIjADAAAAAJAIzAAAAAAAJAIzAAAAAACJwAwAAAAAQCIwAwAAAACQCMwAAAAAACQCMwAAAAAAicAMAAAAAEAiMAMAAAAAkAjMAAAAAAAkAjMAAAAAAInADAAAAABAIjADAAAAAJAIzAAAAAAAJAIzAAAAAACJwAwAAAAAQCIwAwAAAACQCMwAAAAAACQCMwAAAAAAydm21yMAAAAAAPiPBzMAAAAAAInADAAAAABAIjADAAAAAJAIzAAAAAAAJAIzAAAAAACJwAwAAAAAQCIwAwAAAACQCMwAAAAAACQCMwAAAAAAyQWZiLOVdwllugAAAABJRU5ErkJggg==\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ============================================================================\n",
        "#                         ADVANCED CHEAT SHEET\n",
        "# ============================================================================\n",
        "\n",
        "print(\"=\"*70)\n",
        "print(\"              ADVANCED TENSORFLOW/KERAS CHEAT SHEET\")\n",
        "print(\"=\"*70)\n",
        "\n",
        "cheat_sheet = \"\"\"\n",
        "GRADIENTTAPE PATTERNS\n",
        "---------------------\n",
        "# Basic gradient\n",
        "with tf.GradientTape() as tape:\n",
        "    y = model(x)\n",
        "grads = tape.gradient(y, model.trainable_variables)\n",
        "\n",
        "# Higher-order derivatives (nested tapes)\n",
        "with tf.GradientTape() as t2:\n",
        "    with tf.GradientTape() as t1:\n",
        "        y = f(x)\n",
        "    dy = t1.gradient(y, x)\n",
        "d2y = t2.gradient(dy, x)\n",
        "\n",
        "# Jacobian\n",
        "jacobian = tape.jacobian(y, x)\n",
        "\n",
        "# Custom gradient\n",
        "@tf.custom_gradient\n",
        "def custom_op(x):\n",
        "    def grad(dy):\n",
        "        return dy * custom_backward\n",
        "    return forward_result, grad\n",
        "\n",
        "CUSTOM KERAS LAYERS\n",
        "-------------------\n",
        "class CustomLayer(keras.layers.Layer):\n",
        "    def __init__(self, units, **kwargs):\n",
        "        super().__init__(**kwargs)\n",
        "        self.units = units\n",
        "\n",
        "    def build(self, input_shape):\n",
        "        self.kernel = self.add_weight(\n",
        "            shape=(input_shape[-1], self.units),\n",
        "            initializer='glorot_uniform',\n",
        "            trainable=True\n",
        "        )\n",
        "        super().build(input_shape)\n",
        "\n",
        "    def call(self, inputs, training=False):\n",
        "        return inputs @ self.kernel\n",
        "\n",
        "    def get_config(self):\n",
        "        config = super().get_config()\n",
        "        config['units'] = self.units\n",
        "        return config\n",
        "\n",
        "CUSTOM TRAINING\n",
        "---------------\n",
        "# Override train_step for model.fit()\n",
        "class CustomModel(keras.Model):\n",
        "    def train_step(self, data):\n",
        "        x, y = data\n",
        "        with tf.GradientTape() as tape:\n",
        "            y_pred = self(x, training=True)\n",
        "            loss = self.compute_loss(y=y, y_pred=y_pred)\n",
        "        grads = tape.gradient(loss, self.trainable_variables)\n",
        "        self.optimizer.apply_gradients(zip(grads, self.trainable_variables))\n",
        "        return {'loss': loss}\n",
        "\n",
        "GRADIENT MANIPULATION\n",
        "---------------------\n",
        "# Clip by global norm\n",
        "grads, _ = tf.clip_by_global_norm(grads, max_norm=1.0)\n",
        "\n",
        "# Gradient accumulation\n",
        "accumulated = [acc + g/steps for acc, g in zip(accumulated, grads)]\n",
        "\"\"\"\n",
        "print(cheat_sheet)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "z5Z_iXlFWcDA",
        "outputId": "b5c4074a-67bc-4857-897f-44285297a381"
      },
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "======================================================================\n",
            "              ADVANCED TENSORFLOW/KERAS CHEAT SHEET\n",
            "======================================================================\n",
            "\n",
            "GRADIENTTAPE PATTERNS\n",
            "---------------------\n",
            "# Basic gradient\n",
            "with tf.GradientTape() as tape:\n",
            "    y = model(x)\n",
            "grads = tape.gradient(y, model.trainable_variables)\n",
            "\n",
            "# Higher-order derivatives (nested tapes)\n",
            "with tf.GradientTape() as t2:\n",
            "    with tf.GradientTape() as t1:\n",
            "        y = f(x)\n",
            "    dy = t1.gradient(y, x)\n",
            "d2y = t2.gradient(dy, x)\n",
            "\n",
            "# Jacobian\n",
            "jacobian = tape.jacobian(y, x)\n",
            "\n",
            "# Custom gradient\n",
            "@tf.custom_gradient\n",
            "def custom_op(x):\n",
            "    def grad(dy):\n",
            "        return dy * custom_backward\n",
            "    return forward_result, grad\n",
            "\n",
            "CUSTOM KERAS LAYERS\n",
            "-------------------\n",
            "class CustomLayer(keras.layers.Layer):\n",
            "    def __init__(self, units, **kwargs):\n",
            "        super().__init__(**kwargs)\n",
            "        self.units = units\n",
            "\n",
            "    def build(self, input_shape):\n",
            "        self.kernel = self.add_weight(\n",
            "            shape=(input_shape[-1], self.units),\n",
            "            initializer='glorot_uniform',\n",
            "            trainable=True\n",
            "        )\n",
            "        super().build(input_shape)\n",
            "\n",
            "    def call(self, inputs, training=False):\n",
            "        return inputs @ self.kernel\n",
            "\n",
            "    def get_config(self):\n",
            "        config = super().get_config()\n",
            "        config['units'] = self.units\n",
            "        return config\n",
            "\n",
            "CUSTOM TRAINING\n",
            "---------------\n",
            "# Override train_step for model.fit()\n",
            "class CustomModel(keras.Model):\n",
            "    def train_step(self, data):\n",
            "        x, y = data\n",
            "        with tf.GradientTape() as tape:\n",
            "            y_pred = self(x, training=True)\n",
            "            loss = self.compute_loss(y=y, y_pred=y_pred)\n",
            "        grads = tape.gradient(loss, self.trainable_variables)\n",
            "        self.optimizer.apply_gradients(zip(grads, self.trainable_variables))\n",
            "        return {'loss': loss}\n",
            "\n",
            "GRADIENT MANIPULATION\n",
            "---------------------\n",
            "# Clip by global norm\n",
            "grads, _ = tf.clip_by_global_norm(grads, max_norm=1.0)\n",
            "\n",
            "# Gradient accumulation\n",
            "accumulated = [acc + g/steps for acc, g in zip(accumulated, grads)]\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "\n",
        "# Conclusion\n",
        "\n",
        "## Your Advanced TensorFlow & Keras Journey\n",
        "\n",
        "Congratulations! You've mastered advanced TensorFlow and Keras techniques.\n",
        "\n",
        "### What You Learned\n",
        "\n",
        "| Part | Topic | Key Takeaway |\n",
        "|------|-------|-------------|\n",
        "| I | Advanced GradientTape | Nested tapes, Jacobians, custom gradients |\n",
        "| II | Building Ops | Conv, pooling, normalization from scratch |\n",
        "| III | Primitive Layers | Dense, Conv2D with only tf.Variable |\n",
        "| IV | Custom Keras Layers | Proper subclassing with build() and call() |\n",
        "| V | Advanced Architectures | ResNet, SE-Net, Transformer blocks |\n",
        "| VI | Custom Training | Full control with GradientTape |\n",
        "| VII | Practical Demos | Real-world model combining everything |\n",
        "\n",
        "### When to Use What\n",
        "\n",
        "| Approach | Use When |\n",
        "|----------|----------|\n",
        "| `model.fit()` | Standard training, quick prototyping |\n",
        "| Custom `train_step()` | Custom logic but want callbacks/validation |\n",
        "| Full GradientTape loop | GANs, RL, complex multi-model training |\n",
        "| Custom layers | Reusable components, research |\n",
        "| Primitive layers | Learning, debugging, maximum control |\n",
        "\n",
        "### The Complete Learning Path\n",
        "\n",
        "1. **NumPy from Scratch** - Understand the math deeply\n",
        "2. **PyTorch** - Research-friendly framework\n",
        "3. **TensorFlow/Keras Part 1** - Fundamentals and high-level API\n",
        "4. **TensorFlow/Keras Part 2** - Advanced custom components (This notebook!)\n",
        "\n",
        "### Next Steps\n",
        "\n",
        "- **Vision Transformers (ViT)** - Transformers for images\n",
        "- **Diffusion Models** - State-of-the-art generative AI\n",
        "- **Neural Architecture Search** - Automated model design\n",
        "- **Quantization & Pruning** - Model optimization for deployment\n",
        "- **TensorFlow Extended (TFX)** - Production ML pipelines\n",
        "\n",
        "---\n",
        "\n",
        "*\"The more you understand the primitives, the better you can innovate.\"*\n",
        "\n",
        "**Happy Deep Learning!**"
      ],
      "metadata": {
        "id": "wQs6OOHGWcDA"
      }
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.10.0"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}